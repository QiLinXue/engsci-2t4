\documentclass{article}
\usepackage{qilin}
\tikzstyle{process} = [rectangle, rounded corners, minimum width=1.5cm, minimum height=0.5cm,align=center, draw=black, fill=gray!30, auto]
\title{Reading Project: Superconductivity \\ A Random Collection of Notes}
\author{QiLin Xue}  
\date{Fall 2022}
\usepackage{mathrsfs}
\usetikzlibrary{arrows}
\usepackage{stmaryrd}
\usepackage{accents}
\newcommand{\ubar}[1]{\underaccent{\overline}{#1}}
\usepackage{pgfplots}
\numberwithin{equation}{section}
\usetikzlibrary{quantikz}
\usepackage[american]{circuitikz}
\newcommand{\equals}{=}
\usepackage{bbm}
\begin{document}

\maketitle
\tableofcontents
\newpage
\section{Mathematical Background}
\subsection{Gaussian Integrals}
We have the following formulas for gaussian integrals,
\begin{equation}
    \label{eq:gaussian}
    \boxed{\int e^{-x^2/2} \dd{x} = \sqrt{2\pi}}.
\end{equation}
\begin{proof}
    Let $I = \int e^{-x^2/2} \dd{x}.$ By Fubini, we have,
    \begin{align}
        I^2 &= \int\int e^{-(x^2+y^2)/2}\dd{x}\dd{y} \\ 
        &= \int_0^{2\pi}\int_{0}^{\infty} r e^{-r^2/2}\dd{r}\dd{\theta} \\ 
        &= 2\pi,
    \end{align}
    so $I = \sqrt{2\pi}.$
\end{proof}
If we have a linear term, the integral can be generalized to,
\begin{equation}
    \boxed{I(a,b) = \int e^{-ax^2/2-bx}\dd{x} =\exp\left(\frac{b^2}{2a}\right)\sqrt{\frac{2\pi}{a}}}.
\end{equation}
\begin{proof}
    Completing the square, we can write,
    \begin{align}
        \int e^{-ax^2/2-bx}\dd{x} &= \int \exp\left(-\frac{a}{2}\left(x+\frac{b}{a}\right)^2\right)\exp\left(\frac{b^2}{2a}\right) \dd{x}.
    \end{align}
    The constant term factors out and the remaining can be solved using equation \ref{eq:gaussian} and applying u-substitution. 
\end{proof}
Generalizing even further, we can compute,
\begin{equation}
    \boxed{I(a,n) = \int x^n e^{-ax^2/2}\dd{x} = \begin{cases}
        \exp\left(a^{-n/2}\sqrt{\frac{2\pi}{a}}\right) & n\text{ even} \\ 
        0 & n\text{ odd}
    \end{cases}}
\end{equation}
\begin{proof}
    We can take partial derivatives of the simplified expression for $I(a,b)$, and prove via induction that:
    \begin{align}
        \frac{\partial^n}{\partial b^n}I(a,b)\bigg|_{b=0} = \begin{cases}
            a^{-n/2}\sqrt{2\pi/a} & n \text{ even} \\
            0 & n \text{ odd}.
        \end{cases}
    \end{align}
    But we can also take partial derivatives of $I(a,n)$ using the fundamental theorem, that if $n$ is even, we have,
    \begin{equation}
        \frac{\partial^n}{\partial b^n}I(a,b)\bigg|_{b=0} = I(a,n).
    \end{equation}
\end{proof}
Everything can be generalized to multiple dimensions using matrices. For example, using Fubini we can evaluate 
\begin{equation}
    \boxed{I(\lambda_1,\dots,\lambda_d) = \int\cdots \int e^{-\lambda_1x^2/2-\cdots -\lambda_nx_d^2/2}\dd{x_1}\cdots\dd{x_d} = \frac{(2\pi)^{d/2}}{\sqrt{\lambda_1\cdots\lambda_d}}.}
\end{equation}
Given a symmetric, positive definite matrix $A \in \mathbb{R}^{d\times d},$ we have 
\begin{equation}
    \boxed{Z = \int e^{-\bm{x}^T A \bm{x}/2} \dd{\bm{x}} = \sqrt{\frac{(2\pi)^d}{\det A}}.}
    \label{eq:gaussian-nd}
\end{equation}
\begin{proof}
    Since symmetric matrices are orthogonally diagonalizable, we can write $A=S\Lambda S^{-1},$ where $\Lambda$ is diagonal. Then under the change of variables $S_T:x \mapsto S^Tx$ where the Jacobian is $|\det (S^T)'| = \det (S^T) = 1,$ we have,
    \begin{align}
        Z &= \int e^{-1/2 \bm{x}^T \Lambda \bm{x}} \dd{\bm{x}} \\ 
        &= I(\lambda_1,\dots,\lambda_d),
    \end{align}
    which can be evaluated using equation \ref{eq:gaussian-nd}.
\end{proof}
Similarly to the 1-dimensional case, we can also add a linear term and evaluate 
\begin{equation}
    \boxed{Z(\bm{J}) = \int e^{-\bm{x}^TA\bm{x}/2 - \bm{J}^T\bm{x}}\dd{\bm{x}} = \sqrt{\frac{(2\pi)^d}{\det A}}\exp\left(\frac{1}{2}\bm{J}^TA^{-1}\bm{J}\right)}.
\end{equation}
\begin{proof}
    We can complete the square to write and expand,
    \begin{align}
        Z(\bm{J}) &= \int \exp\left(-\frac{1}{2}\lambda_i x_i^2 - (\bm{J}^TS^T)_i x_i\right) \dd{\bm{x}} \\ 
        &= \prod_{i}\sqrt{\frac{2\pi}{\lambda_i}}\exp\left(\frac{(J^TS^T)_i^2}{2\lambda_i}\right).
    \end{align}
    The first factor is easy to evaluate, and the exponential factor can be evaluated using,
    \begin{align}
        \frac{(\bm{J}^TS^T)_i^2}{2\lambda_i} &= \frac{1}{2 \lambda_i}\left((\bm{J}^T)_a(S^T)^{a}{}_i\right)^2 \\ 
        &= \frac{1}{2}(\bm{J}^T)_a(S^T)^{a}{}_i \frac{1}{\lambda_i} S_{i}{}^b(\bm{J}^T)_{b} \\ 
        &= \frac{1}{2}(\bm{J}^T)_a(S^T)^{a}{}_i \left(S_{ik}(A^{-1})^{k\ell}(S^T)_{\ell i}\right) S_{i}{}^b(\bm{J}^T)_{b} \\ 
        &= \frac{1}{2}(\bm{J}^T)_k (A^{-1})^{k\ell} (\bm{J}^T)_{\ell} \\
        &= \frac{1}{2}\bm{J}^T A^{-1}\bm{J},
    \end{align}
    where we used the facts that 
    \begin{equation}
        \frac{1}{\lambda_i} = S_{ik}(A^{-1})^{k\ell}(S^T)_{\ell i},
    \end{equation}
    as well as 
    \begin{equation}
        S_{ab}S_{bc}^T = \delta_{ac}
    \end{equation}
    for the second last step.
\end{proof}
Using this result, we can determine 
\begin{equation}
    \boxed{\langle x_{i_1}\cdots x_{i_n}\rangle_0 = \frac{1}{Z(0)}x_{i_1}\cdots x_{i_n}e^{-\bm{x}^TA\bm{x}/2}\dd{\bm{x}} = \frac{\partial}{\partial \bm{J^T}_{i_1}}\cdots \frac{\partial}{\partial \bm{J^T}_{i_d}} \bigg|_{\bm{J}=0} (-1)^n \exp\left(\frac{1}{2}\bm{J}^TA^{-1}\bm{J}\right).}
\end{equation}
\begin{proof}
    Differentiating inside the integral sign and differentiating the result gives us the above. The trick is that each $\frac{\partial}{\partial (\bm{J^T}_{i_k})}$ brings down a factor of $x_k.$
\end{proof}
\subsection{Correlation Functions}
\begin{theorem}
    \emf{Wick's Theorem} can be written in the form 
    \begin{equation}
        \langle x_{i_1}x_{i_2}\cdots x_{i_{2n}}\rangle_0 = \sum_\pi \langle x_{i_{\pi(1)}}x_{i_{\pi(2)}}\rangle_0  \cdots \langle x_{i_{\pi(2n-1)}}x_{i_{\pi(2n)}}\rangle_0,
    \end{equation}
    where $\pi \in S_{2n}.$
\end{theorem}
To make this useful, note that 
\begin{equation}
    \boxed{\langle x_ix_j\rangle_0 = (A^{-1})_{ij}.}
\end{equation}
Sometimes, we may write the functioning generator as 
\begin{equation}
    Z(\bm{J}) = Z(0)\exp\left(\frac{1}{2}\bm{J}^TG\bm{J}\right),
\end{equation}
instead such that we take on the slightly simpler expression of 
\begin{equation}
    \langle x_ix_j\rangle_0 = G_{ij}.
\end{equation}
\section{Ginzburg-Landau Theory of Superconductivity}
Consider a potential 
\begin{equation}
    V(\Delta) = t|\Delta|^2 + u|\Delta|^4,
\end{equation}
which will be derived in the future. Here, $\Delta$ is the energy required to break a cooper pair. When $t$ goes from positive to negative, it signifies a phase transition, where
\begin{equation}
    t = \frac{T-T_c}{T_c}.
\end{equation}
\begin{center}
    \begin{tikzpicture}
        \begin{axis}[
        legend pos=outer north east,
        title=Potential $V(|\Delta|)$,
        axis lines = middle,
        xlabel = $|\Delta|$,
        ylabel = $V$,
        variable = t,
        trig format plots = rad,
        ]
        \addplot [
            domain=-1.1:1.1,
            samples=70,
            color=blue,
            ]
            {x^2 + x^4};
        \addlegendentry{$t>0$}
        \addplot [
            domain=-1.1:1.1,
            samples=70,
            color=blue,
            ]
            {-x^2 + x^4};
        \addlegendentry{$t<0$}
        \end{axis}
        \end{tikzpicture}
\end{center}
For $t>0,$ then $\Delta_* =0.$ For $t<0,$ we have $|\Delta_*|^2 = \frac{-t}{2u}.$ Therefore,
\begin{equation}
    V(\Delta_*) = \begin{cases}
        0 & t>0 \\ 
        -\frac{t^2}{4u} & t<0.
    \end{cases}
\end{equation}
We can use this to determine what the specific heat capacity is (which is something that is measurable). The specific heat capacity is 
\begin{equation}
    C = -\frac{\partial^2 F}{\partial t^2}.
\end{equation}
If we assume that we only have potential energy, we have $F = V,$ which gives 
\begin{equation}
    C = \begin{cases}
        0 & t>0 \\ 
        \frac{!}{2u} & t< 0,
    \end{cases}
\end{equation}
so experimentally there will be a discontinuity. Furthermore, the power law relationship that 
\begin{equation}
    |\Delta| \sim |t|^{1/2}
\end{equation}
can be experimentally verified.

\section{Quantum Mechanics in 3D}
The Laplacian in spherical coordinates is 
\begin{equation}
    \nabla^2 = \frac{1}{r^2}\frac{\partial}{\partial r}\left(r^2 \frac{\partial}{\partial r}\right) + \frac{1}{r^2\sin\theta}\frac{\partial}{\partial\theta}\left(\sin\theta\frac{\partial}{\partial\theta}\right) + \frac{1}{r^2\sin^2\theta}\frac{\partial^2}{\partial\phi^2}.
\end{equation}
Suppose we're given a potential $V(\bm{x})=V(r),$ so the SchrÃ¶dinger equation gives 
\begin{equation}
    \frac{1}{r^2}\frac{\partial}{\partial r}\left(r^2\frac{\partial\Psi}{\partial r}\right) + \frac{1}{r^2\sin\theta} \frac{\partial}{\partial \theta}\left(\sin\theta \frac{\partial \Psi}{\partial \theta}\right) + \frac{1}{r^2\sin^2\theta} \frac{\partial^2\Psi}{\partial \varphi^2} - \frac{2m}{\hbar}V(r)\Psi = -\frac{2mE}{\hbar^2}\Psi.
\end{equation}
Using separation of variables, we can write $\Psi = R(r)Y(\theta,\varphi),$ so we can write,
\begin{align}
    &\frac{\partial}{\partial r}\left(r^2 \frac{\partial \Psi}{\partial r}\right) + \frac{1}{\sin\theta}\frac{\partial}{\partial\theta}\left(\sin\theta \frac{\partial\Psi}{\partial \theta}\right) + \frac{1}{\sin^2\theta}\frac{\partial^2\Psi}{\partial\varphi^2} + \frac{(E-V)2mr^2\Psi}{\hbar^2}=0 \\ 
\implies & \frac{\partial}{\partial r}(r^2R'Y) + \frac{1}{\sin\theta}\frac{\partial}{\partial\theta}\left(\sin\theta \frac{\partial r}{\partial\theta}\right)R + \frac{R}{\sin^2\theta}\frac{\partial^2Y}{\partial\varphi^2}+\frac{(E-V)2mr^2RY}{\hbar^2} RY = 0.
\end{align}
Dividing both sides by $RY$ gives 
\begin{equation}
    \frac{1}{R}\frac{\partial}{\partial r}(r^2R') + \frac{(E-V)2mr^2}{\hbar^2} + \frac{1}{Y\sin\theta}\frac{\partial}{\partial\theta}\left(\sin\theta\frac{\partial Y}{\partial\theta}\right) + \frac{1}{Y\sin^2\theta}\frac{\partial^2Y}{\partial\varphi^2}=0.
\end{equation}
The first part depends only on $r,$ and is valid for all $r,$ and the second part depends on and only on $\theta,\varphi,$ so they must both be constants. That is,
\begin{align}
    \frac{1}{Y\sin\theta}\frac{\partial}{\partial\theta}\left(\sin\theta \frac{\partial Y}{\partial \theta}\right) + \frac{1}{Y\sin^2\theta}\frac{\partial^2 Y}{\partial\varphi^2} = -\ell(\ell + 1), 
\end{align}
where $\ell$ is a constant. Therefore, 
\begin{align}
    \frac{1}{R}\frac{\partial}{\partial r}(r^2R') + \frac{(E-V)2mr^2}{\hbar^2} = \ell(\ell + 1),
\end{align}
which is the radial equation, and the previous equation is the angular equation. We can rewrite the radial equation as,
\begin{align}
    -\frac{\hbar^2}{2mr^2}\frac{\partial}{\partial r}\left(r^2 \frac{\partial R}{\partial r}\right) + \left(V+\frac{\hbar^2\ell(\ell+1)}{2mr^2}\right)R=ER,
\end{align}
where we can interpret the first term as the kinetic energy and the second term can be interpreted as the effective potential energy. In problem set three, we are asked to find the bound state for a particle in the potential 
\begin{equation}
    V = \begin{cases}
        -V_0 & r<a \\ 
        0 & \text{ otherwsie}
    \end{cases}.
\end{equation}
A bound state will have $E<0.$ It so happens that $\ell=0$ is the minimum value of $\ell,$ which corresponds to the smallest energy. This gets us 
\begin{align}
    &-\frac{\hbar^2}{2mr^2}\frac{\partial}{\partial r}\left(r^2\frac{\partial R}{\partial r}\right) + VR = ER. 
\end{align}
We can make the substitution $u = rR,$ we can solve. We can compute,
\begin{equation}
    \frac{\partial u}{\partial r} = \frac{\partial (rR)}{\partial r} = R + r\frac{\partial R}{\partial r}.
\end{equation}
This is an important problem because it shows that there is some minimum $V_0$ that allows for bound states. However, in a metal which has fermions (i.e. superconductors), $V_0$ can be infinitesimally small, and bound states can still be formed, which is remarkable. In superconductors, these bound states correspond to cooper pairs, which are required for superconductors.
\section{Free Fermi Gas}
The simplest model for a metal is found by ignoring both the lattice structure and the Coulomb force. We only keep the fact that we have electrons which are fermions.

Consider a cube-shaped box of side length $L$ such that $V=L^3,$ and put $N$ electrons in it. The Hamiltonian is then 
\begin{equation}
    \hat{H} = \sum_{i=1}^N \frac{\vec{p}_i^2}{2m},
\end{equation}
since there is no potential energy. Note that we have not yet taken into account that they are fermions. We wish to solve $\hat{H}\psi = E\psi,$ where 
\begin{equation}
    \psi = \psi(\vec{x}_1,\vec{x}_2,\dots,\vec{x}_N).
\end{equation}
Technically speaking, we still need to take into account the spin, but will do so later. We have,
\begin{align}
    H\psi = \sum_{i=1}^N -\frac{\hbar^2}{2m}\nabla_i^2 \psi = E\psi.
\end{align}
Applying separation of variables, we have
\begin{align}
    \psi(\vec{x}_1,\vec{x}_2,\dots,\vec{x}_N) = \prod_{i=1}^N \psi_i(\vec{x}_i),
\end{align}
which is possible when there are no interactions. This gives,
\begin{equation}
    H\psi = -\frac{\hbar^2}{2m}\sum_{i=1}^{N}\psi_1(\vec{x}_1)\cdots (\nabla_i^2 \psi_i(\vec{x}_i)) \cdots \psi_N(\vec{x}_N) = E\psi_1(\vec{x}_1)\cdots \psi_N(\vec{x}_N).
\end{equation}
Dividing both sides by $\psi,$ we get 
\begin{equation}
    \frac{H\psi}{\psi} = E = -\frac{\hbar^2}{2m} \sum_{i=1}^N \frac{\nabla_i^2 \psi_i(\vec{x}_i)}{\psi_i(\vec{x}_i)}.
\end{equation}
Observe that each term in the sum must be a constant. Then, we can identify 
\begin{equation}
    E_i = -\frac{\hbar^2}{2m}\frac{\nabla_i^2\psi_i(x_i)}{\psi_i}
\end{equation}
and $E = \sum_{i=1}^N E_i.$ Oftentimes people will say that a noninteracting problem is the same as a one-particle problem, which is certainly true in this case. For a 1-particle problem, we can apply separation of variables again,
\begin{equation}
    E\psi = -\frac{\hbar^2}{2m} \left(\frac{\partial^2\psi}{\partial x^2} + \frac{\partial^2\psi}{\partial y^2} + \frac{\partial^2\psi}{\partial z^2}\right). 
\end{equation}
Let us write $\psi(x,y,z)=X(x)Y(y)Z(z),$ giving,
\begin{equation}
    E = -\frac{\hbar^2}{2m}\left( \frac{X''}{X} + \frac{Y''}{Y} + \frac{Z''}{Z}\right).
\end{equation}
We can solve the three differential equations separately, with the \emf{periodic boundary conditions,}
\begin{align}
    X(x) &= X(x+L) \\ 
    Y(x) &= Y(x+L) \\ 
    Z(x) &= Z(x+L).
\end{align}
Note that in some texts, periodic boundary conditions refer to $f(x)=f(x+L),$ which is more general, but in this case they are equivalent since the Hamiltonian is translation invariant. The equations we which to solve are,
\begin{align}
    -\frac{\hbar^2}{2m}\frac{X''}{X} &= E_x \\ 
    -\frac{\hbar^2}{2m}\frac{Y''}{Y} &= E_y \\
    -\frac{\hbar^2}{2m}\frac{Z''}{Z} &= E_z.
\end{align}
This gives,
\begin{align}
    X'' + \frac{2mE_x}{\hbar^2}X &= 0,
\end{align}
so 
\begin{align}
    X =\{\sin(k_xx),\cos(k_x,x)\} = \{e^{ik_xx},e^{-ik_xx}\},
\end{align}
where $k_x^2 = \frac{2mE_x}{\hbar^2}.$ The periodic boundary conditions put a restriction on what $k_x$ can be. This gives us 
\begin{equation}
    e^{\pm ik_xx} = e^{\pm ik_x(x+L)} \implies k_xL = 2\pi n_x,
\end{equation}
where $n_x$ is an integer. Note that we account for both $\pm ik_xn$ by allowing $n_x$ to be positive or negative. The same is true for the $Y,Z$ equations,
\begin{align}
    k_x=\frac{2\pi n_x}{L},\quad\quad\quad k_y=\frac{2\pi n_y}{L},\quad\quad\quad k_z=\frac{2\pi n_z}{L},
\end{align}
so the wavevector is 
\begin{equation}
    \vec{k} = \frac{2\pi}{L}(n_x,n_y,n_z),
\end{equation}
and the energy for a single particle is,
\begin{equation}
    E = E_x+E_y+E_z = \frac{\hbar^2}{2m} \sqrt{k_x^2+k_y^2+k_z^2}.
\end{equation}
Now we can take into account these electrons are fermions.
\begin{itemize}
    \item Ground state $E=0:$ $2$ particles can occupy it.
    \item First excited state $E_1:$ $3\cdot 2$ particles can occupy it.
    \item $n$th excited state: $E_n:$ $2n$ particles can occupy it.
\end{itemize}
If we have $N$ electrons, then they in the ground state of the system, they will occupy all the lowest energy states, such that no two particles are in the same quantum state.

In $k$-space, each point $(k_x,k_y,k_z)$ occupies a box space of volume $\left(\frac{2\pi}{L}\right)^3.$ In $k$-space, a sphere of radius $k_F$ has volume 
\begin{equation}
    \frac{4}{3}\pi k_F^3,
\end{equation}
which has a total of 
\begin{align}
    \frac{N}{2} \cdot \frac{8\pi^3}{L^3} = \frac{4}{3}\pi k_F^3 \implies N = \frac{1}{3\pi^2} L^3k_F^3,
\end{align}
or 
\begin{equation}
    k_F = \left(\frac{N}{V}3\pi^2\right)^{1/3},
\end{equation}
where $k_F$ is the \emf{Fermi wave vector} and
\begin{equation}
    E_F = \frac{\hbar^2k_F^2}{2m}
\end{equation}
is the \emf{Fermi energy}, and the hypersphere in $k$-space is the \emf{Fermi surface.}
\begin{idea}
    Oftentimes, we need to evalute 
    \begin{equation}
        \sum_{\vec{k} \in FS} f(E(\vec{k})),
    \end{equation}
    where $FS$ is the Fermi surface and $E(\vec{k}) = \frac{\hbar^2\vec{k}^2}{2m}.$ This can be a pain, but we can use the fact that 
    \begin{equation}
        \int \delta(\epsilon - E(\vec{k})) \dd{\epsilon} = 1.
    \end{equation}
    This allows us to rewrite,
    \begin{align}
        \sum_{\vec{k}}f(E(\vec{k})) &= \int \underbrace{\rho(E)}_{\approx \text{ const}} f(E) \dd{E} \\ 
        &= \rho \int f(E) \dd{E}.
    \end{align}
\end{idea}
\section{2nd Quantization}
The partition function in quantum mechanics is 
\begin{equation}
    Z = \sum_n e^{-\beta E_n} = \text{tr }e^{-\beta \hat{H}}.
\end{equation}
For a quantum harmonic oscillator, i.e. $E_n=\hbar \omega(n+1/2),$ the partition function is 
\begin{align}
    Z &= \sum_n e^{-\beta E_n} \\ 
    &= e^{-\frac{1}{2}\beta \hbar\omega}\sum_n \left(e^{-\beta \hbar \omega}\right)^n \\ 
    &= \boxed{\frac{e^{-\frac{1}{2}\beta \hbar\omega}}{1-e^{-\beta \hbar \omega}}}.
\end{align}
The expectation value can be written as 
\begin{equation}
    \langle \hat{A}\rangle  = \frac{1}{Z}\text{tr}\left(\hat{A}e^{-\beta \hat{H}}\right).
\end{equation}
Subatomic particles are either fermions (which obey the Pauli exclusion principle), and bosons.
\begin{itemize}
    \item Electrons are fermions
    \item Photons are bosons
    \item Phonons are bosons
\end{itemize}
Note that no two fermions can be in the same quantum state. For example, consider the two states $\ket{\uparrow}$ and $\ket{\downarrow}.$ If we think from a chemistry perspective, no two fermions can be in the $s$ shell with the same spin. This is a \textit{consequence} of the antisymmetry of the wavefunction. Namely, for fermions 
\begin{equation}
    \psi(x_1,x_2) = -\psi(x_2,x_1),
\end{equation}
and for bosons,
\begin{equation}
    \psi(x_1,x_2) = \psi(x_2,x_1).
\end{equation}
For \emf{second quantization,} we want to build a Hilbert space for many identical quantum particles. We can define \emf{sectors.}
\begin{idea}
    In many particle quantum mechanics, the number of particles is not a conserved quantity. For example, we can have annhilation and creation of particles (converting it from energy to mass and back). In superconductors, we typically have a bath of electrons where we can freely take and put back electrons.
\end{idea}
Instead, we can define \emf{Sectors,} each with a well-defined number of particles. For fermions,
\begin{itemize}
    \item 0 particles: $\ket{0}$ is the \emf{vacuum vector}
    \item 1 particle: $\{\ket{i,\sigma}\}_{i=1,\dots,N_s,\sigma=\uparrow,\downarrow}$ where $N_s$ is the number of sites and $\sigma$ is the site. We are assuming that there is one occupiable orbital site. This gives $2N_s$ single particle states.
    \item 2 particles: $\ket{i\sigma}\ket{j\tau}:$ The naive guess is $(2N_s)^2$ states, but we have to account for the fact that they can't occupy the same state. Instead, there are 
    \begin{equation}
        \binom{2N_s}{2} = \frac{(2N_s)(2N_s-1)}{2} 
    \end{equation}
    states.
    \item $n$ particles: We have 
    \begin{equation}
        \binom{2N_s}{n}
    \end{equation}
    states, which gets very big, very fast. This is another reason why single-particle quantum mechanics can only get us so far.
\end{itemize}
We wish to define operators which bring us between different sectors. We can define the \emf{creation operator}
\begin{equation}
    c^\dagger_{i\sigma}\ket{\sigma} = \ket{i\sigma}
\end{equation}
and the inverse is the \emf{annihilation operator}, where
\begin{equation}
    c_{i\sigma}c_{i\sigma} = \ket{0}.
\end{equation}
Note these identities, 
\begin{align}
    c_{i\sigma}\ket{0} &= 0 \\ 
    (c_{i\sigma})^2 &= 0 \\ 
    (c^\dagger_{i\sigma}) &= 0.
\end{align}
Furthermore, we can deal with the antisymmetric nature of fermions by noting that 
\begin{equation}
    c^\dagger_{i\sigma}c^\dagger_{j\sigma'} = -c^\dagger_{j\sigma'}c^\dagger_{i\sigma},
\end{equation}
so
\begin{equation}
    \ket{(i\sigma)(j\sigma')} = -\ket{(j\sigma')(i\sigma)}.
\end{equation}
In general, if we define the anti-commutator $\{A,B\}=AB+BA,$ then 
\begin{equation}
    \{c_{i\sigma},c^\dagger_{j\sigma'}\} = \delta_{ij}\delta_{\sigma\sigma'}.
\end{equation}
Furthermore, annhilation and creation operators anti-commute with themselves, i.e. 
\begin{align}
    \{c_{i\sigma}, c_{j\sigma'}\} &= 0 \\
    \{c^\dagger_{i\sigma}, c^\dagger_{j\sigma'}\} &= 0.
\end{align}
For \textit{bosons,} we have 
\begin{align}
    [b_{i\sigma},b_{j\sigma}^\dagger] &= \delta_{ij}\delta_{\sigma\sigma'} \\
    [b_{i\sigma},b_{j\sigma}] &= 0 \\
    [b_{i\sigma}^\dagger,b_{j\sigma}^\dagger] &= 0,
\end{align}
where $[A,B]=AB-BA.$

Usually, we can think of an analogy with the quantum harmonic oscillator, where 
\begin{equation}
    \hat{H} = \hbar\omega\left(\underbrace{a^\dagger a}_{n} + \frac{1}{2}\right),
\end{equation}
where we can treat $a^\dagger a$ as the energy level. As an analogy, we can treat the system as only one energy level, and each new particle we add on a constant energy. This is what motivates the creation and annihilation operators.
\begin{problem}[3.3.1]
    We can prove the following,
    \begin{enumerate}[label=(\alph*)]
        \item $[\hat{a},\hat{a}^\dagger] = 1$ since $$\hat{a}\hat{a}^\dagger \ket{n} = \sqrt{n+1}\hat{a}\ket{n+1} = (n+1)\ket{n}$$ and 
        $$\hat{a}^\dagger \hat{a}\ket{n} = \hat{a}^\dagger\sqrt{n}\ket{a-1} = n\ket{n},$$ so the commutator of the two will give $[\hat{a},\hat{a}^\dagger]\ket{n} = \ket{n}.$
        \item $\hat{H} = \hbar\omega\left(\hat{a}^\dagger\hat{a} + \frac{1}{2}\right)$ is true since we've already shown that $\hat{a}^\dagger\hat{a} = n.$ But in order to prevent circular logic in the future, we can write,
        \begin{align}
            \hat{a}^\dagger\hat{a} &= \frac{1}{2m\hbar\omega}\left(m\omega \hat{x}-i\hat{p}\right)\left(m\omega \hat{x}+i\hat{p}\right) \\ 
            &= \frac{1}{2m\hbar\omega}\left(\hat{p}^2 + m^2\omega^2\hat{x}^2 + m\omega i [\hat{x},\hat{p}]\right) \\
            &= \frac{1}{\hbar\omega}\left(\frac{\hat{p}^2}{2m} + \frac{1}{2}m\omega^2\hat{x}\right) - \frac{1}{2}.
        \end{align}
        Substituting this into $\hbar\omega(\hat{a}^\dagger\hat{a} + \frac{1}{2})$ gives us the desired relationship.
        \item $[\hat{H},\hat{a}^\dagger]=\hbar\omega\hat{a}^\dagger$ since
        \begin{align}
            \hat{H}\hat{a}^\dagger \ket{n} &= \hat{H}\sqrt{n+1}\ket{n+1}\\ 
            &= \frac{1}{n+1}\hat{a}^\dagger \hat{a}\sqrt{n+1}E_{n+1}\ket{n+1} \\ 
            &= \hat{a}^\dagger E_{n+1}\ket{n},
        \end{align}
        and 
        \begin{align}
            \hat{a}^\dagger \hat{H}\ket{n} &= \hat{a}^\dagger E_n \ket{n}.
        \end{align}
        Recall that $E_{n+1} - E_n  = \hbar\omega,$ which proves this. We can show similarly that $[\hat{H},\hat{a}]=-\hbar\omega\hat{a}.$
        \item We already proved this in the previous problem, where we can identify $E_{n+1} = E_n + \hbar\omega.$
        \item Using part (b), the eigenvalues of $\hat{H}$ are directly related to the eigenvalues of $\hat{a}^\dagger\hat{a},$ so we get 
        \begin{equation}
            E_n = \frac{\hbar\omega}{2}\left(n+\frac{1}{2}\right).
        \end{equation}
        \item To show $\hat{a}\ket{n} = \sqrt{n}\ket{n-1}$ and $\hat{a}^\dagger\ket{n} = \sqrt{n+1}\ket{n+1},$ we can compute,
        \begin{align}
            \hat{a}\ket{n} &= \frac{1}{\sqrt{2m\hbar\omega}}(m\omega\hat{x}+i\hat{p})\ket{n }
        \end{align}
    \end{enumerate}
\end{problem}
\section{Time Evolution}
In quantum mechanics, an initial wavefunction $\ket{\psi}$ evolves through time according to the Schrodinger equation,
\begin{align}
    i\hbar\partial_t\ket{\psi} = H\ket{\psi}.
\end{align}
This means that 
\begin{equation}
    \ket{\psi(t)} = e^{-iHt/\hbar}\ket{\psi(0)}.
\end{equation}
We can extract physical information about the system by using expectation values of operators, i.e. 
\begin{equation}
    \langle A\rangle = \bra{\psi(t)}A\ket{\psi(t)}.
\end{equation}
This is the \emf{Schrodinger picture}
\begin{itemize}
    \item States depend on time
    \item Operators are independent of time.
\end{itemize}
Alternatively, we can use the \emf{Heisenberg picture,} wherein states are time independent but operators are time dependent, i.e. 
\begin{align}
    \langle A\rangle &= \ket{\psi}e^{iHt/\hbar}Ae^{-iHt/\hbar}\ket{\psi},
\end{align}
where $A_H(t)$ is operator, with the $H$ standing for Heisenberg. This can be nice because we can make conceptual connections to classical mechanics. Normally, to understand how the position of a particle evolves through time in classical mechanics, we just need to find $x(t)$. With the Heisenberg picture, we can compute 
\begin{equation}
    \hat{x}(t) = e^{i\hbar{H}t/\hbar}\hat{x}e^{-i\hbar{H}t/\hbar}.
\end{equation}
\begin{example}
    Consider the quantum harmonic oscillator,
    \begin{equation}
        \hat{H} = \frac{1}{2m}\hat{P}^2 + \frac{1}{2}m\omega^2\hat{x}^2,
    \end{equation}
    and wish to find $\hat{x}(t).$ To do this, we need a mathematical trick,
    \begin{equation}
        e^{A}Be^{-A} = \sum_{n=0}^{\infty}\frac{1}{n!}[A,[A,\dots,[A,B]]],
    \end{equation}
    where the commutators are nested $n$ times.
    \begin{proof}
        Let $f(s) = e^{sA}Be^{-sA}.$ Its Taylor expansion is
        \begin{align}
            f(s) &= \sum_{n=0}^{\infty} \frac{1}{n!}s^n f^{(n)}(0),
        \end{align}
        where
        \begin{align}
            f(0) &= B \\ 
            f'(0) &= e^{sA}ABe^{-sA}+e^{sA}B(-A)e^{-sA} \\ 
            &= e^{sA}[A,B]e^{-sA} \\ 
            f''(0) &= e^{sA}A[A,B]e^{-sA}+e^{sA}[A,B](-A)e^{-sA} \\
            &= e^{sA}[A,[A,B]]e^{-sA},
        \end{align}
        and we can inductively prove the initial statement by setting $s=1.$
    \end{proof}
    Using this, we can compute,
    \begin{align}
        \hat{x}(t) = \sum_{n=0}^{\infty}\left(\frac{it}{\hbar}\right)^n \frac{1}{n!}[H,[H,\dots,[H,x]\dots]].
    \end{align}
    Recall that $[H,x] = -\frac{i\hbar p}{m}$ and $[H,p] = i\hbar m\omega^2 x.$ Therefore, the nested commutators give us something proportional to $p,$ for an odd number of commutators and for an even number of commutators, it is proportional to $x.$ This then leads to something in the form of 
    \begin{equation}
        \dot{x}(t) = A\hat{x}\cos\omega t + B\hat{p}\sin\omega t
    \end{equation}
\end{example}
\section{Coherent States}
A \emf{coherent state} of a QHO is a state which is the most \textit{classical}. They satisfy 
\begin{equation}
    \hat{a}\ket{\alpha} = \alpha\ket{\alpha},
\end{equation}
where $\hat{a}$ is the annihilation operator. To find the position space wavefunction of a state $\psi$, we compute
\begin{equation}
    \bra{x}\ket{\psi}=\psi(x).
\end{equation}
\begin{example}
    To find the position space wavefunction of the ground state of the ground state of the QHO, $\ket{0},$ we note that $\hat{a}\ket{0} = 0$ (as it is the definition). We have 
    \begin{align}
        & \bra{x}a\ket{0}=0\\ 
        \implies & \bra{x}(i\hat{p}+m\omega \hat{x})\ket{0} = 0 \\ 
        \implies & i\bra{x}\hat{p}\ket{0} + m\omega\bra{x}\hat{x}\ket{0} = 0 \\
        \implies & \hbar \frac{\partial}{\partial x}\psi_0 + m\omega x\psi_0 = 0.
    \end{align}
    Solving this differential equation gives us 
    \begin{equation}
        \psi_0(x) = Ae^{-m\omega x^2/2\hbar},
    \end{equation}
    where $A$ is the normalization factor.
\end{example}
A hint for problem set 4, 2.2(a)ii,
\begin{itemize}
    \item \textit{Hint:} Consider
    \begin{align}
        \bra{x}\hat{a}\ket{\alpha(t)}&=\bra{x}\hat{a}e^{-iHt/\hbar}\ket{\alpha} \\ 
        &= \bra{x}e^{-iHt/\hbar}e^{iHt/\hbar}\hat{a}e^{-iHt/\hbar}\ket{\alpha},
    \end{align}
    and work from there.
\end{itemize}
\section{Bloch Theorem}
\textit{NB:Missed one lecture on lattices. Stuff might out of nowhere.}
If an electron moves in a periodic potential $V(\bm{x})=V(\bm{x}+\bm{R}),$ where $\bm{R}$ is the \emf{Bravais Lattice vector}.

The wavefunction can be written as 
\begin{align}
    \psi_{\bm{k}}(\bm{x}) &= e^{i\bm{k}\cdot \bm{x}} u_{\bm{k}}(\bm{x}),
\end{align}
whereby $u_{\bm{k}}(\bm{x}+\bm{R})=u_{\bm{k}}(\bm{x}).$
\begin{proof}
    Recall that $[T_{\bm{R}},H]=0,$ where $T_{\bm{R}}$ is the translation operator, because $T$ is a symmetry of the Hamiltonian. This implies that $T_R,H$ can be simultaneously diagonalised, i.e.
    \begin{align}
        T_{\bm{R}}\psi(\bm{x}) &= \psi(\bm{x}+\bm{R}) \\ 
        &= c(\bm{R})\psi(\bm{x}),
    \end{align}
    and 
    \begin{align}
        H\psi_{\bm{k}}(\bm{x}) &= \epsilon_{\bm{k}}\psi_{\bm{k}}(\bm{x}).
    \end{align}
    Observe that 
    \begin{align}
        T_RT_{R'} &= \psi(x+\bm{R}+\bm{R'}) \\ 
        &= c(\bm{R}+\bm{R'})\psi(x) \\ 
        &= c(\bm{R})c(\bm{R'})\psi(x).
    \end{align}
    Note that the exponential function has the same property as $c,$ so we can hope to write 
    \begin{equation}
        c(\bm{R}) = e^{i\bm{k}\cdot \bm{R}}.
    \end{equation}
    Let us write 
    \begin{equation}
        \bm{R} = n_1\bm{a}_1+n_2\bm{a}_2+n_3\bm{a}_3.
    \end{equation}
    Then,
    \begin{equation}
        c(\bm{R})=c^{n_1}(a_1)c^{n_2}(a_2)c^{n_3}(a_3).
    \end{equation}
    This justifies the dot product form. Specifically, we also know that (periodic boundary condition)
    \begin{align}
        \psi(\bm{x}+N_1\bm{a}_1) &= \psi(\bm{x}) \\ 
        &= [c(a_1)]^{N_1}\psi(x),
    \end{align}
    so 
    \begin{equation}
        c(a_1) = e^{ i \cdot \frac{2\pi n_1}{N_1}} = e^{i\frac{2\pi}{L}x_i},
    \end{equation}
    where $x_i=n_ia$ and $L_x=N_1|a_1|.$ We can do this for the $y$ and $z$ directions, to get 
    \begin{equation}
        \bm{k} = (2\pi n_1/L_x, 2\pi n_2/L_y, 2\pi n_3/L_z),
    \end{equation}
    which is true for a simple lattice. For a generic lattice, we have,
    \begin{equation}
        \bm{k} = \frac{n_1}{N_1}\bm{g}_1 + \frac{n_2}{N_2}\bm{g}_2 + \frac{n_3}{N_3}\bm{g}_3,
    \end{equation}
    We can now define 
    \begin{align}
        u_{\bm{k}}(\bm{x}) &= e^{-i\bm{k}\cdot \bm{x}}\psi_{\bm{k}}(\bm{x}) \\
        T_{\bm{R}}u_{\bm{k}}(\bm{x}) &= e^{-i\bm{k}\cdot (\bm{x}+\bm{R})} \psi_{\bm{k}}(\bm{x}+\bm{R}) \\ 
        &= u_{\bm{k}}(\bm{x})
    \end{align}
    See page 134 of A\&M for more details.
\end{proof}
An interesting consequence: Let $\bm{G}$ be the reciprocal lattice vector, which sort of acts like the dual. Namely, given $\{\bm{a}_1,\bm{a}_2,\bm{a}_3\},$ we can define $\{\bm{g}_1,\bm{g}_2,\bm{g}_3\}$ such that 
\begin{equation}
    \bm{a}_i\cdot \bm{g}_j = 2\pi \delta_{ij}.
\end{equation}
We then have,
\begin{align}
    T_{\bm{R}}\psi_{\bm{k}+\bm{G}} &= e^{i(\bm{k}+\bm{G})\cdot \bm{R}}\psi_{\bm{k}+\bm{G}}(\bm{x}) \\ 
    &= e^{i\bm{k}\cdot \bm{R}}\psi_{\bm{k}+\bm{G}}(\bm{x}).
\end{align}
Since the same thing happens for $\bm{k}$ and $\bm{k}+\bm{G}$, we may restrict $\bm{k}$ to the \emf{First Brillouin Zone (FBZ).} In reciprocal lattice space, e.g. a square lattice, we have 
\begin{align}
    \bm{a}_1 &= (a,0) \\ 
    \bm{a}_2 &= (0,a) \\ 
    \bm{g}_1 &= \tilde{a}(1,0) \\ 
    \bm{g}_2 &= \tilde{a}(0,1),
\end{align}
where $\tilde{a} = \frac{2\pi}{a}.$ Therefore, we can construct an equivalency class and restrict $\bm{k}$ to 
\begin{align}
   \bm{k} \in [-\tilde{a}/2,\tilde{a}/2]^2.
\end{align}
\section{Fourier Transform}
Any function on the lattice may be Fourier transformed. We have,
\begin{align}
    f(\bm{R}_i) &= \frac{1}{\sqrt{N}}\sum_{\bm{k} \in FBZ}e^{i\bm{k}\cdot \bm{R}_i}\tilde{f}(\bm{k}).
\end{align}
Note that we can recover $f(\bm{R}_i+N_i\bm{a}_i)$ since 
\begin{equation}
    \bm{k}\cdot \bm{R}_1 = \bm{k} \cdot N_1\bm{a}_1 = \frac{n_1}{N_1}(2\pi N_1) = 2\pi n_1.
\end{equation}
The inverse Fourier transform is given by 
\begin{equation}
    \tilde{f}(\bm{k}) = \frac{1}{\sqrt{N}}\sum_i e^{-i\bm{k}\cdot \bm{R}_i}f(\bm{R}_i).
\end{equation}
Let us now apply the Fourier transform. Recall that for a 2D square lattice, we have the Hamiltonian
\begin{equation}
    H = -t\sum_{\langle i,j\rangle} (c^\dagger_{i\sigma}c_{j\sigma} + c^\dagger_{j\sigma}c_{i\sigma}),
\end{equation}
where $c^\dagger_{i\sigma}$ crates an $e^{-}$ on site $i$ with spin $\sigma$ and $c^{\dagger}_{i\sigma}c_{j\sigma}$ moves an electron from site $j$ to site $i.$ And $\langle i,j\rangle$ sums over all nearest neighbour pairs of sites. We have, after a change of basis (i.e. going to a delocalized basis)
\begin{align}
    c_{i\sigma} &= \frac{1}{\sqrt{N}}\sum_{\bm{k}}e^{i\bm{k}\cdot \bm{R}_i}c_{\bm{k}\sigma} \\ 
    c^{\dagger}_{i\sigma} &= \frac{1}{\sqrt{N}}\sum_k e^{-i\bm{k}\cdot \bm{R}_i}c^{\dagger}_{\bm{k}\sigma},
\end{align}
where $c^\dagger_{\bm{k}\sigma}$ creates an electron with wavevector $\bm{k}$ and spin $\sigma.$ Therefore, this effectively \emf{delocalizes} the electrons from their respective sites. Using this, we can rewrite the Hamiltonian as 
\begin{align}
    H &= -\frac{t}{N}\sum_{\langle i,j\rangle}\sum_{\bm{k},\bm{k'}} c^{\dagger}_{\bm{k}\sigma}c_{\bm{k}'\sigma}e^{-i\bm{k}\bm{R}_i}e^{i\bm{k'}R_j} + \text{h.c.} \\
    &= -\frac{t}{N} \sum_{\bm{k},\bm{k'}} c^{\dagger}_{\bm{k}\sigma}c_{\bm{k}'\sigma}\left(\sum_{\langle i,j\rangle} e^{-i\bm{k}\cdot \bm{R}_i}e^{i\bm{k}'\cdot \bm{R}_j}\right) + \text{h.c.} \\
    &=-\frac{t}{N} \sum_{\bm{k},\bm{k'}} c^{\dagger}_{\bm{k}\sigma}c_{\bm{k}'\sigma}\sum_i\left(e^{-i(\bm{k}-\bm{k'})\bm{R}_i}\left[e^{i\bm{k'}\cdot\bm{a}_1}+e^{i\bm{k'}\cdot \bm{a}_2}\right]\right) + \text{h.c}\\ 
    &= -\frac{t}{N}\sum_{k,k'}c^{\dagger}_{\bm{k}\sigma}c_{\bm{k}'\sigma} N \delta_{\bm{k},\bm{k}'}\left(e^{i\bm{k'}\cdot \bm{a}_1}+e^{i\bm{k'}\cdot \bm{a}_2}\right) + \text{h.c.} \\
    &= -t\sum_{\bm{k}}\left[\left(e^{i\bm{k}\cdot \bm{a}_1} + e^{i\bm{k}\cdot \bm{a}_2}\right)c^{\dagger}_{\bm{k}\sigma}c_{\bm{k}\sigma}+hc\right] \\ 
    &= -t\sum_k \left(e^{ika_1}+e^{ika_2}+e^{-ika_1}+e^{-ika_2}\right)c^{\dagger}_{\bm{k}\sigma}c_{\bm{k}\sigma} \\
    &= -2t\sum_k \left(\cos(\bm{k}\cdot \bm{a}_1) + 2\cos(\bm{k}\cdot \bm{a}_2)\right)c^{\dagger}_{\bm{k}\sigma}c_{\bm{k}\sigma}.
\end{align}
We can define 
\begin{equation}
    \epsilon_{\bm{k}} = -2t\sum_k \left(\cos(\bm{k}\cdot \bm{a}_1) + 2\cos(\bm{k}\cdot \bm{a}_2)\right),
\end{equation}
such that 
\begin{equation}
    H = \sum_{k\sigma}\epsilon_{\bm{k}}c^{\dagger}_{\bm{k}\sigma}c_{\bm{k}\sigma}.
\end{equation}
\section{Action in Path Integral}
Suppose we have a classical simple harmonic oscillator and suppose that it is at $x_a$ at time $t_a$ and $x_b$ at time $t_b.$ We can show that the action is given by 
\begin{equation}
    S = \frac{m\omega}{2\sin(\omega T)}\left[(x_a^2+x_b^2)\cos(\omega T) - 2x_ax_b\right].
\end{equation}
\begin{proof}
    We first find the desired motion. Recall that we have 
    \begin{equation}
        x(t) = A\cos(\omega t) + B\sin(\omega T).
    \end{equation}
    We have a system of two equations:
    \begin{align}
        x_a &= A\cos(\omega t_a) + B\sin(\omega t_a) \\ 
        x_b &= A\cos(\omega t_b) + B\sin(\omega t_b).
    \end{align}
    We can solve for $A$ using elimination,
    \begin{align}
        x_a\sin(\omega t_b) -x_b\sin(\omega t_a) &= A\cos(\omega t_a)\sin(\omega t_b)  - A\cos(\omega t_b)\sin(\omega t_a),
    \end{align}
    and solve 
    \begin{align}
        A &= \frac{x_a\sin(\omega t_b) - x_b\sin(\omega t_a)}{\cos(\omega t_a)\sin(\omega t_b) - \cos(\omega t_b)\sin(\omega t_a)} \\ 
        B &= \frac{x_a\cos(\omega t_b) - x_b\cos(\omega t_a)}{\sin(\omega t_a)\cos(\omega t_b) - \sin(\omega t_b)\cos(\omega t_a)}.
    \end{align}
    We can then substitute this into the action,
    \begin{align}
        S &= \int_{t_a}^{t_b} \left(\frac{1}{2}m\dot{x}^2 - \frac{1}{2}m\omega^2 x^2\right) \dd{t} \\ 
        &= \int_{t_a}^{t_b} \left(\frac{1}{2}m \left(-\omega A\sin(\omega t) + \omega B\cos(\omega t)\right)^2 - \frac{1}{2}m\omega^2 \left(A\cos(\omega t) + B\sin(\omega t)\right)^2\right) \dd{t}
    \end{align}
\end{proof}
Recall that in quantum mechanics, we have the following time evolution 
\begin{align}
    \ket{\psi(t_b)} = e^{-i\hat{H}(t_b-t_a)/\hbar}\ket{\psi(t_a)}.
\end{align}
\begin{proof}
    We start from the time dependent S.E,
    \begin{equation}
        i\hbar \frac{\partial \ket{\psi}}{\partial t} = \hat{H}\ket{\psi}.
    \end{equation}
    We multiply both sides by $e^{i\hat{H}t/\hbar}.$ Then,
    \begin{align}
        i\hbar e^{i\hat{H}t/\hbar}\frac{\partial \ket{\psi}}{\partial t} = e^{i\hat{H}t/\hbar} \hat{H}\ket{\psi}
    \end{align}
    Moving everything to one side, we can rewrite it as 
    \begin{equation}
        \frac{\partial}{\partial t}\left(e^{iHt/\hbar}\ket{\psi(t)}\right) = 0.
    \end{equation}
    This must be a constant $\ket{\phi}$, so 
    \begin{equation}
        \ket{\psi(t)} = e^{-i\hat{H}t/\hbar}\ket{\phi},
    \end{equation}
    and we can use initial conditions to finish up.
\end{proof}
We can write $\psi(x_b,t_b)$ as an integral equation involving the position space wave function $\psi(x_a,t_a)$. We have,
    \begin{align}
        \bra{x_b}\ket{\psi(t_b)} &= \bra{x_b}e^{-iH(t_b-t_a)/\hbar}\ket{\psi(t_a)} \\ 
        &=\int \underbrace{\bra{x_b}e^{-iH(t_b-t_a)/\hbar}\ket{x_a}}_{K(x_b,t_b;x_a,t_a)}\underbrace{\bra{x_a}\ket{\psi(t_a)}}_{\psi(x_a,t_a)}\dd{x} \\ 
        &= \int K(x_b,t_b;x_a,t_a)\psi(x_a,t_a)\dd{x}.
    \end{align}
    where we used the fact that 
    \begin{equation}
        \mathbb{I} = \int \ket{x}\bra{x} \dd{X}.
    \end{equation}
    Here, $K$ is the \emf{kernel} (different from the one in linalg) and can be thought of as the weight of a particle traveling from $x_a$ at $t_a$ to $x_b$ at $t_b.$ We can write the kernel as a path integral. To do this, we first write 
    \begin{equation}
        K(x,t,x',t') = \bra{x}U(t,t')\ket{x'},
    \end{equation}
    and define $T\equiv t-t'.$ We can rewrite 
    \begin{equation}
        e^{-iHT/\hbar} = (e^{-iH\epsilon/\hbar})^N,
    \end{equation}
    where $\epsilon = \frac{T}{N}.$ We want to take the limit as $N\to \infty.$ Substituting this back in, we have 
    \begin{align}
        K = \bra{x}e^{-iH\epsilon/\hbar}\cdots e^{-iH\epsilon/\hbar}\ket{x'}.
    \end{align}
    There are $N$ factors that appear, so we can insert $N-1$ identities in between. We have,
    \begin{align}
        K = \bra{x}e^{-iH\epsilon/\hbar}\mathbbm{1}e^{-iH\epsilon /\hbar}\mathbbm{1}\cdots \mathbbm{1}e^{-iH\epsilon/\hbar}\ket{x'},
    \end{align}
    which gives us $N-1$ integrals, 
    \begin{equation}
        K = \iint \bra{x}e^{-iH\epsilon/\hbar}\ket{x_{N-1}}\bra{x_{N-1}}e^{-iH\epsilon/\hbar}\cdots \bra{x_1}e^{-iH\epsilon/\hbar}\ket{x'} \dd{x}_1\cdots \dd{x}_{N-1}.
    \end{equation}
    Let's analyze
    \begin{align}
        \bra{x}_ne^{-iH\epsilon/\hbar}\ket{x_{n-1}} &= \bra{x_n}\exp{-i\frac{p^2}{2m}\frac{\epsilon}{\hbar} - i\frac{\hat{V}\epsilon}{\hbar}}\ket{x_{n-1}} \\ 
        &=\bra{x_n}\exp{-i\frac{p^2}{2m}\frac{\epsilon}{\hbar}} \exp{-i\frac{\hat{V}\epsilon}{\hbar}}\ket{x_{n-1}}.
    \end{align}
    Note that this second step is only true for $\epsilon \ll 1,$ since the error is on the order of $O(\epsilon^2).$ Going back to the big path integral, we have 
    \begin{equation}
        V(\hat{x})\ket{x_{n-1}} = V(x_{n-1})\ket{x_{n-1}},
    \end{equation}
    so 
    \begin{equation}
        e^{-iV(\hat{x})\epsilon / \hbar}\ket{x_{n-1}} = e^{-iV(x_{n-1})\epsilon/\hbar}\ket{x_{n-1}}.
    \end{equation}
    % Therefore,
    % \begin{align}
    %     K &= \iint \bra{x}e^{-i\hat{p}^2\hbar/ 2m}\ket{x_{N-1}}\bra{x_{N-1}}e^{-i\hat{p}^2\hbar/ 2m}\ket{x_{N-2}}\cdots e^{-iV(x_{n-1})\epsilon/\hbar}e^{-iV(x_{n-2})\epsilon/\hbar}\cdots \dd{x}_1\cdots \dd{x}_{N-1}.
    % \end{align}
    We can also write the identity as 
    \begin{equation}
        \mathbbm{1} = \iint \ket{p}\bra{p} \dd{p}.
    \end{equation}
    This gives us 
    \begin{equation}
        \bra{x_n}e^{-ip^2\epsilon / 2m\hbar}\mathbbm{1}\ket{x_{n-1}} = \bra{x_n}e^{-ip^2\epsilon / 2m\hbar}\ket{p}\bra{p}\ket{x_{n-1}},
    \end{equation}
    where 
    \begin{equation}
        \bra{p}\ket{x_{n-1}} = \frac{1}{\sqrt{2\pi \hbar}}e^{-ipx_{n-1}/\hbar}.
    \end{equation}
    Using this, we have 
    \begin{align}
        \bra{x_n}e^{-i\hat{p}^2\epsilon/2m\hbar}\ket{x_{n-1}} &= \int \bra{x_n}\ket{p}e^{-ip^2\epsilon/2m\hbar} \frac{e^{-px_{n-1}/\hbar}}{\sqrt{2\pi\hbar}}\dd{p} \\ 
        &= \int \frac{1}{2\pi\hbar}e^{-i(x_n-x_{n-1})p/\hbar}e^{-ip^2\epsilon / 2m\hbar}.
    \end{align}
    Note:
    \begin{equation}
        \int_{-\infty}^{\infty} e^{\pm i r^2}\dd{r} = e^{\pm i\pi/4}\sqrt{\pi}.
    \end{equation}
    Completing the square, we can compute this to be 
    \begin{align}
        \bra{x_n}e^{-i\hat{p}^2\epsilon/2m\hbar}\ket{x_{n-1}} &= \frac{1}{2\pi \hbar}e^{im(x_n-x_{n-1})^2/2\hbar\epsilon}\int \dd{p} e^{-i\epsilon/2m\hbar (p-m/\epsilon (x_n-x_{n-2})^2)} \\ 
        &= \frac{1}{2\pi \hbar}e^{im(x_n-x_{n-1})^2/2\hbar\epsilon}\int \dd{p} e^{-i(\epsilon/2m\hbar) p^2} \\ 
        &= e^{im(x_n-x_{n-1})^2/(2\hbar\epsilon)}\frac{1}{2\pi \hbar}\sqrt{\frac{\pi 2m\hbar}{\epsilon i}}.
    \end{align}
    Bringing back the potential term, we obtain 
    \begin{equation}
        \boxed{\bra{x_n}e^{-iH\epsilon /\hbar}\ket{x_{n-1}} = e^{im(x_n-x_{n-1})^2/(2\hbar\epsilon)} e^{-iV(x_{n-1})\epsilon /\hbar}\sqrt{\frac{m}{2\pi \hbar i \epsilon}}}
    \end{equation}
    Finally, we can simplify $K$ as 
    \begin{align}
        K &= \int \prod \sqrt{\frac{m}{2\pi i\epsilon \hbar}}e^{i\frac{m}{2\hbar\epsilon}(x_n-x_{n-1})^2 - i V(x_{n-1}\epsilon/\hbar)} \\ 
        &= \int \left(\frac{m}{2\pi i \epsilon\hbar}\right)^{N/2} \exp\left(\frac{i\epsilon}{\hbar}\left[\sum_{n=1}^N \frac{m}{2}\left(\frac{x_n-x_{n-1}}{\epsilon}\right)^2 - V(x_{n-1})\right]\right) \dd^{N-1}x \\ 
        &= \int \exp\left(\frac{i}{\hbar} \int_{t_a}^{t_a}\left(\frac{1}{2}m\dot{x}^2 - V(x(t))\right)\dd{t}\right) \mathcal{D}x \\ 
        &= \int \exp\left(\frac{i}{\hbar} \int_{t_a}^{t_a}\mathcal{L}\dd{t}\right) \mathcal{D}x \\ 
        &= \int e^{iS/\hbar}\mathcal{D}x
    \end{align}
    where $x_N=x$ and $x_0=x'.$ For the last sum, recall that we can set $\epsilon \to \dd{t}$ when we take the limit as $N\to \infty.$ Note that when integrating over an infinite number of variables, we denote
    \begin{equation}
        \mathcal{D}x = \lim_{N\to \infty}\left(\frac{m}{2\pi i \hbar\epsilon}\right)^{N/2} \dd{x}_1\cdots \dd{x}_{N-1}.
    \end{equation}
    The quantity $K=\int e^{iS/\hbar}\mathcal{D}x$ is the \emf{path integral}.
    \section{Hubbard Model}
    Let us model a single atom with one orbital. If we view it using the Bohr model (which is surprisingly accurate, as electrons in a solid are very closely bound to their nucleus), we can write the Hamiltonian as 
    \begin{equation}
        H_\text{site}= Un_\uparrow n_\downarrow + \epsilon_a(n_\uparrow + n_\downarrow),
    \end{equation}
    where $n_\uparrow$ and $n_\downarrow$ are the number of electrons in the up and down spin states, respectively. The first term is the Coulomb interaction between the electrons, and the second term is the interaction energy between the electrons. 
    
    For 2 atoms interacting where an electron can hop between atoms, we have 
    \begin{equation}
        H_\text{hop} = -t(c_{i\sigma}^\dagger c_{j\sigma} + c_{j\sigma}^\dagger c_{i\sigma}),
    \end{equation}
    where $t$ is an energy known as the hopping amplitude and $\sigma$ is the spin. Recall that $c_{i\sigma}^\dagger$ creates an electron in the $i$th site with spin $\sigma$ and $c_{i\sigma}$ annihilates an electron in the $i$th site with spin $\sigma.$

    We can move to the total model, we have:
    \begin{equation}
        H = -t\sum_{\langle i,j\rangle, \sigma}(c_{i\sigma}^\dagger c_{j\sigma} + c_{j\sigma}^\dagger c_{i\sigma}) + Un_{i\uparrow} n_{j\downarrow} - \epsilon_a(n_\uparrow + n_\downarrow),
    \end{equation}
    where $\langle i,j\rangle$ denotes a pair of nearest neighbors. Note that usually, the last term can b ignored as it is just a constant.

    \textbf{Problem:} For superconductivity, we want attraction between two electrons, and the above model makes it very difficult for two electrons to get close to each other.

    To solve this problem, we can use the \emf{attractive Hubbard model}, which is
    \begin{equation}
        H = -t\sum_{\langle i,j\rangle, \sigma}(c_{i\sigma}^\dagger c_{j\sigma} + c_{j\sigma}^\dagger c_{i\sigma}) - Un_{i\uparrow} n_{j\downarrow}.
    \end{equation}
    There is a real physical reason for this change in sign, which will be covered later. Let's study this model! Specifically, we wish to compute the partition function $Z.$

    Here's the fun part! We want to compute 
    \begin{equation}
        Z = \tr e^{-\beta \hat{H}} = \sum_n e^{-\beta E_n}
    \end{equation}
    This method is completely hopeless, because it's just too hard lmao. To do this, we need several identities and concepts to write $Z$ as a path integral. Recall that $c_{i\sigma}, c_{i\sigma}^\dagger$ obey the following relationships:
    \begin{align}
        \{c_{i\sigma}, c_{j\sigma}^\dagger\} &= \delta_{ij}\delta_{\sigma\sigma'} \\
        \{c_{i\sigma}, c_{j\sigma'}\} &= 0 \\ 
        \{c_{i\sigma}^\dagger, c_{j\sigma'}^\dagger\} &= 0.
    \end{align}
    Define \emf{Grassmann numbers} to commute with elements in $\mathbb{C}$ but anticommute with $\hat{c},\hat{c}^\dagger$ and also anticommute with each other. Here are some neat properties,
    \begin{itemize}
        \item Their exponential is very well-behaved, i.e. 
        \begin{equation}
            e^c = 1+c
        \end{equation}
        since higher order terms go to zero as they anticommute.
        \item Similarly,
        \begin{equation}
            e^{\overline{c}c} = 1 + \overline{c}c.
        \end{equation}
        \item We also have 
        \begin{equation}
            \frac{1}{1-\overline{c}c} = 1+ \overline{c}c
        \end{equation}
    \end{itemize}
    We define calculus with Grassmann numbers,
    \begin{align}
        &\frac{d}{dc}1=0,\quad\quad\quad \frac{d}{dc}c=1,\\ 
        &\int 1 \dd{c} = 0\quad\quad\quad \int c \dd{c} = 1.
    \end{align}
    We can use this to compute,
    \begin{align}
        \int e^{ac} \dd{c} &= \int 1 + ac \dd{c}  \\ 
        &= a.
    \end{align}
    and 
    \begin{align}
        \int e^{-a\overline{c}c}\dd{\overline{c}}\dd{c} &= \int 1 - a\overline{c}c \dd{\overline{c}}\dd{c} \\
        &= -a\int c \dd{c} \\ 
        &= -a.
    \end{align}
    \section{Coherent State Path Integral for Partition Function}
    Consider creation and annihilation operator for a fermion $\hat{c}^\dagger$. We have,
    \begin{align}
        \hat{c}^\dagger \ket{0} &= \ket{1} \\
        \hat{c}^\dagger \ket{1} &= 0 \\
        \hat{c} \ket{0} &= 0 \\
        \hat{c} \ket{1} &= \ket{0}.
    \end{align} 
For this lecture, we will be very careful with the $\hat{c}$ operator and the $c$ as the Grassmann number. We can define 
\begin{equation}
    \boxed{\ket{c} = \ket{0} - c\ket{1}}.
\end{equation}
We can compute,
\begin{align}
    \hat{c}\ket{c} &= \hat{c}\ket{0} +c\hat{c}\ket{1} \\ 
    &= 0 + c\ket{0},
\end{align}
where we used the fact that $c$ anticommutes with $\hat{c}$. We can also write this as 
\begin{align}
    \hat{c}\ket{c} = c(\ket{0} - c\ket{1}) = c\ket{c},
\end{align}
where we used the fact that $c^2=0$ since $c$ is a Grassmann number. Therefore, $\ket{c}$ is an eigenstate of $\hat{c}$ with eigenvalue $c.$ 

A state which is the eigenstate of the annhilation operator is called a coherent state. Therefore, $\ket{c}$ is a \emf{fermionic coherent state}. 

We can now begin to construct the path integral for the partition function. By definition, $Z=\text{Tr}(e^{-\beta\hat{H}}).$ Then:
\begin{align}
    \text{tr}A &= \int \bra{-\overline{c}}A\ket{c} e^{-\overline{c}c}\dd{\overline{c}}\dd{c}
\end{align}
where $A$ is some combination of $\hat{c},\hat{c}^\dagger.$ We can use the fact that 
\begin{equation}
    \bra{-\overline{c}} = \bra{0} - \bra{1}\hat{c}.
\end{equation}
Then,
\begin{align}
    Z &= \int \bra{\overline{c}_0}e^{-\beta\hat{H}}\ket{c_0} e^{-\overline{c}_0c_0}\dd{\overline{c}_0}\dd{c_0}.
\end{align}
Similarly to the previous path integral formulation, we write $e^{-\beta \hat{H}} = (e^{-\epsilon H})^N$ where $\epsilon = \frac{\beta}{N}.$ This is 
\begin{equation}
    Z = \int \bra{\overline{c}_0}e^{-\epsilon H}e^{-\epsilon H}\cdots e^{-\epsilon H}\ket{c_0} e^{-\overline{c}_0c_0}\dd{\overline{c}_0}\dd{c_0}.
\end{equation}
Using the identity 
\begin{equation}
    \mathbb{I} = \int \ket{c}\bra{\overline{c}} e^{-\overline{c}c}\dd{\overline{c}}\dd{c}.
\end{equation}
Then, we can just insert multiple copies of this into the integral $N-1$ times,
\begin{align}
    Z &= \int \bra{-\overline{c}_0}e^{-\epsilon H}\mathbb{I}e^{-\epsilon H}\cdots e^{-\epsilon H}\ket{c_0} e^{-\overline{c}_0c_0}\dd{\overline{c}_0}\dd{c_0} \\
    &= \int \bra{-\overline{c}_0}e^{-\epsilon \hat{H}}\ket{c_{N-1}}\bra{\overline{c}_{N-1}}e^{-\epsilon \hat{H}} \cdots \ket{c_1}\bra{\overline{c}_1}e^{-\epsilon \hat{H}}\ket{c_0}e^{-\overline{c}_0c_0-\overline{c}_1c_1-\cdots - \overline{c}_{N-1} c_{N-1}}\dd{\overline{c}_0}\dd{c_0}\dd{\overline{c}_1}\dd{c_1}\cdots\dd{\overline{c}_{N-1}}\dd{c_{N-1}}.
\end{align}
Where we define $\overline{c}_N = -\overline{c}_0.$ Then,
\begin{align}
    Z = \int \prod_{n=1}^N \left(\bra{\overline{c}_n} e^{-\epsilon \hat{H}} \ket{c_n} \right)e^{-\overline{c}_0c_0-\overline{c}_1c_1-\cdots - \overline{c}_{N-1} c_{N-1}}\dd{\overline{c}_0}\dd{c_0}\dd{\overline{c}_1}\dd{c_1}\cdots\dd{\overline{c}_{N-1}}\dd{c_{N-1}}.
\end{align}
Because $\epsilon \hat{H} \ll 1,$ we can write 
\begin{equation}
    e^{-\epsilon \hat{H}} \sim 1- \epsilon \hat{H}[\hat{c}^\dagger, \hat{c}],
\end{equation}
where we have explicitly written out the dependence on the Grassmann numbers. Therefore,
\begin{align}
    \bra{\overline{c}_n} \left(1 - \epsilon \hat{H}[\hat{c}^\dagger, \hat{c}]\right) \ket{c_n} &=\bra{\overline{c}_n} \left(1 - \epsilon \hat{H}[\overline{c}_n, \overline{c}_{n-1}]\right) \ket{c_n}  \\ 
    &= \bra{\overline{c}_{n-1}}\ket{c_n}\left(1 - \epsilon H[\overline{c}_n, c_{n-1}]\right) \\ 
    &= e^{\overline{c}c'} e^{-\epsilon H[\overline{c}_n, c_{n-1}]}.
\end{align}
Therefore,
\begin{equation}
    \bra{\overline{c}_n}e^{-\epsilon \hat{H}[\hat{c}^\dagger,\hat{c}]}\ket{c_{n-1}} = e^{\overline{c}_nc_{n-1}}e^{-\epsilon H[\overline{c}_n, c_{n-1}]}.
\end{equation}
Putting this into $Z$ gives 
\begin{align}
    Z &= \int \prod_{n=1}^N \left(e^{\overline{c}_nc_{n-1}}e^{-\epsilon H[\overline{c}_n, c_{n-1}]}e^{-\overline{c}_nc_n}\right) \dd{\overline{c}_0}\dd{c_0}\dd{\overline{c}_1}\dd{c_1}\cdots\dd{\overline{c}_{N-1}}\dd{c_{N-1}} \\
    &= \int \exp\left(-\left(\sum_{n=1}^N \overline{c}_n(c_{n}-c_{n-1})\epsilon H[\overline{c}_n, c_{n-1}]\right)\right) \dd{\overline{c}_0}\cdots \dd{c_{N-1}} \\
    &= \int \exp\left(-\left[\sum_{n=1}^N\overline{c}_n \cdot \frac{\overline{c}_n-c_{n-1}}{\epsilon} + H[\overline{c}_n, c_{n-1}]\right]\epsilon\right)\dd{\overline{c}_0}\cdots \dd{c_{N-1}} \\
    &= \int e^{-\int_0^\beta (\overline{c}\partial_\tau c + H)\dd{\tau}} \mathcal{D}\overline{c}\mathcal{D}c,
\end{align}
where $\epsilon = \beta/N$ and $\tau$ is the imaginary time.
\section{Hubbard-Stratonovish Transformation of the Hubbard Model}
Recall that
\begin{equation}
    \hat{H} = -t\sum_{\langle i,j\rangle,\sigma} (\hat{c}^\dagger_{i\sigma}\hat{c}_{j\sigma} + \hat{c}^\dagger_{j\sigma}\hat{c}_{i\sigma}) - U\sum_i \hat{n}_{i\uparrow}\hat{n}_{i\downarrow}.
\end{equation}
We will introduce the superconducting order parameter, i.e. the \emf{gap}. This is done with the Hubbard-Stratonovish transformation. To do this, first compute:
\begin{align}
    \int e^{-\frac{1}{U}|\Delta|^2 - \bar{c}_\uparrow \bar{c}_\downarrow\Delta - c_{\downarrow}c_{\uparrow}\Delta^*} \dd{\Delta^*}\dd{\Delta} &= \int \exp\left(-\frac{1}{U}\Delta_r^2 - \frac{1}{U}\Delta_i^2 - (\bar{c}_\uparrow\bar{c}_\downarrow + {c}_\uparrow {c}_\downarrow) \Delta_r  - (\bar{c}_\uparrow \bar{c}_\downarrow - c_\downarrow c_\uparrow)i\Delta_i\right) \dd{\Delta_r}\dd{\Delta_i} \\ 
    &= \int \exp\left(-\frac{1}{U}\Delta_r^2-C\Delta_r\right)\dd{\Delta_r}\int \exp\left(-\frac{1}{U}\Delta_i^2-iD\Delta_i\right)\dd{\Delta_i} \\ 
    &= \pi u \exp\left(UC^2/4\right)\exp\left(-UD^2/4\right),
\end{align}
where 
\begin{equation}
    C^2-D^2 = (\bar{c}_\uparrow\bar{c}_\downarrow+c_\uparrow c_\downarrow)^2 - (\bar{c}_\uparrow\bar{c}_\downarrow+c_\uparrow c_\downarrow)^2 = 4\bar{c}_\uparrow\bar{c}_\downarrow c_\uparrow c_\downarrow
\end{equation}
so the integral is equal to
\begin{equation}
    \pi U \exp\left(U\bar{c}_\uparrow\bar{c}_\downarrow c_\uparrow c_\downarrow\right).
\end{equation}
Note that to compute this, we used the fact that
\begin{equation}
    \dd{\Delta^*}\dd{\Delta} = \dd{\Re\Delta}\dd{\Im\Delta},
\end{equation}
where we can substitute $\Delta = \Delta_r + i\Delta_i.$ We can write our partition function as 
\begin{align}
    \mathcal{Z} = \int e^{-S_0-S_\text{int}} \mathcal{D}\bar{c}\mathcal{D}c,
\end{align}
where $S_0$ is the action from the previous section and 
\begin{align}
    S_\text{int} &= - \int_0^\beta U\sum_i \bar{c}_{i\uparrow}\bar{c}_{i\downarrow}c_{i\downarrow}c_{i\uparrow} \dd{\tau} \\ 
    &= \int e^{-\frac{1}{U}\int_0^\beta \sum_i |\Delta_{i\tau}|^2 \dd{\tau} - \int_0^\beta \sum_i(\bar{c}_{i\tau\uparrow}\bar{c}_{i\tau\downarrow}\Delta_{i\tau} + c_{i\tau\downarrow}c_{i\tau \uparrow}\Delta^*_{i\tau}) \dd{\tau}} \mathcal{D}\Delta^*\mathcal{D}\Delta.
\end{align}
The partition function is then 
\begin{equation}
    \mathcal{Z} = \int e^{-S_0-\frac{1}{U}\int_0^\beta \sum_i |\Delta_{i\tau}|^2 \dd{\tau} - \int_0^\beta \sum_i(\bar{c}_{i\tau\uparrow}\bar{c}_{i\tau\downarrow}\Delta_{i\tau} + c_{i\tau\downarrow}c_{i\tau \uparrow}\Delta^*_{i\tau}) \dd{\tau}}\mathcal{D}\Delta^*\mathcal{D}\Delta\mathcal{D}\bar{c}\mathcal{D}c.
\end{equation}
Note that our motivation for $S_{int}$ came from the Hubbard model, where $n=\bar{c}c.$ Transforming $S_{int}$ is known as the Hubbard-Stratonovish transformation.
\vspace{2mm}

Let us try to actually do this integral. We can swap the order of integration,
\begin{align}
    \mathcal{Z} &= \int e^{-\frac{1}{U}\int_0^\beta |\Delta_{i\tau}|^2}\left(\int e^{-S_0}e^{-\int_0^\beta \sum_i(\bar{c}_{i\tau\uparrow}\bar{c}_{i\tau\downarrow}\Delta_{i\tau} + c_{i\tau\downarrow}c_{i\tau \uparrow}\Delta^*_{i\tau}) \dd{\tau}} \mathcal{D}\bar{c}\mathcal{D}c\right)\mathcal{D}\Delta^*\mathcal{D}\Delta \\ 
    &= {\mathcal{Z}_0}\int e^{-\frac{1}{U}\int_0^\beta |\Delta_{i\tau}|^2}\left(\frac{1}{Z_0}\int e^{-S_0}e^{-\int_0^\beta \sum_i(\bar{c}_{i\tau\uparrow}\bar{c}_{i\tau\downarrow}\Delta_{i\tau} + c_{i\tau\downarrow}c_{i\tau \uparrow}\Delta^*_{i\tau}) \dd{\tau}} \mathcal{D}\bar{c}\mathcal{D}c\right)\mathcal{D}\Delta^*\mathcal{D}\Delta
\end{align}
where the $\mathcal{Z}_0 = \int e^{-S_0} \mathcal{D}\bar{c}\mathcal{D}c$ is the partition function of the free fermions without the interaction term. Recall that
\begin{equation}
    \langle A\rangle_0 = \frac{1}{\mathcal{Z}_0}\int Ae^{-S_0}\mathcal{D}\bar{c}\mathcal{D}c.
\end{equation}
We can use this to write 
\begin{align}
    \mathcal{Z} = \mathcal{Z}_0\int e^{-\frac{1}{U}\int_0^\beta |\Delta_{i\tau}|^2} \left\langle e^{-\int_0^\beta \sum_i(\bar{c}_{i\tau\uparrow}\bar{c}_{i\tau\downarrow}\Delta_{i\tau} + c_{i\tau\downarrow}c_{i\tau \uparrow}\Delta^*_{i\tau}) \dd{\tau}}  \right\rangle_0 \mathcal{D}\Delta^*\mathcal{D}\Delta.
\end{align}
\section{Green Functions}
Suppose we have a partition function in the form of 
\begin{equation}
    Z = \int e^{-t|\Delta|^2}\dd{\Delta^*}\dd{\Delta}.
\end{equation}
If $t>0,$ we have that $Z$ converges and if $t<0$ we have that $Z$ diverges. The big idea is that in this form, we can set $t=0$ to get where the phase transition occurs. Continuing from last section, we wish to write:
\begin{equation}
    Z = Z_0 \int e^{-S_{eff}[\Delta^*,\Delta]}\mathcal{D}\Delta^*\mathcal{D}\Delta,
\end{equation}
where $S_\text{eff} \sim t|\Delta|^2 + \Delta^4 + \cdots.$ Note that the $t$ here is temperature, not the $t$ from the Hubbard model. We want to relate this to parameters used in the Hubbard model. Looking at this, we have 
\begin{equation}
    e^{-S_\text{eff}} = e^{-\int_0^\beta \sum_i \frac{|\Delta|^2}{U}\dd{\tau}} \langle e^{-S'}\rangle,
\end{equation}
where we define $S' = \int_0^\beta \sum_i(\bar{c}_{i\tau\uparrow}\bar{c}_{i\tau\downarrow}\Delta_{i\tau} + c_{i\tau\downarrow}c_{i\tau \uparrow}\Delta^*_{i\tau}) \dd{\tau}.$ Taking the log of both sides, we have 
\begin{equation}
    S_\text{eff} = \int_0^\beta \sum_i \frac{|\Delta_{i\tau}|^2}{U}\dd{\tau} - \log \langle e^{-S'}\rangle_0.
\end{equation}
Recall from the first problem set that 
\begin{align}
    \log\langle e^A\rangle = \sum_{n=1}^{\infty} \frac{r^n}{n!}\langle B^n(x) \rangle_c,
\end{align}
where $\langle B^n(x) \rangle_c$ is the \emf{cumulant,} define by 
\begin{align}
    \langle B(x)\rangle_c &= \langle B(x) \rangle \\
    \langle B(x)^2\rangle_c &= \langle B(x)^2\rangle - \langle B(x)\rangle^2.
\end{align}
Now how do we compute the cumulants $\langle \cdots \rangle$? To do this, suppose that 
\begin{equation}
    Z = \int e^{-\bm{x}^T\bm{A}\bm{x}/2} \dd^d{\bm{x}}.
\end{equation}
Then,
\begin{align}
    \langle x_i,x_j\rangle &= \frac{1}{Z} \int x_i x_j e^{-\frac{1}{2}\bm{x}^T\bm{A}\bm{x}}\dd^d{\bm{x}} \\ 
    &= (A^{-1})_{ij}.
\end{align}
And:
\begin{align}
    \langle x_ix_jx_kx_{\ell}\rangle &= \text{sum of all pair expectation values} \\ 
    &= \langle x_ix_j\rangle \langle x_kx_\ell\rangle + \langle x_i x_k \rangle \langle x_jx_\ell \rangle + \langle x_ix_\ell \rangle \langle x_jx_k\rangle \\ 
    &= (A^{-1})_{ij}(A^{-1})_{k\ell} + (A^{-1})_{ik}(A^{-1})_{j\ell} + (A^{-1})_{i\ell}(A^{-1})_{jk}.
\end{align}
However, now instead of real numbers, we want to deal with Grassmann numbers! Suppose we have 
\begin{equation}
    Z = \int e^{-S} \mathcal{D}\bar{c}\mathcal{D}c,\quad\quad\quad\quad S = \int_0^\beta (\bar{c}_\tau \partial_\tau c_\tau + \bar{c}_\tau c_\tau) \dd{\tau}.
\end{equation}
We are going to do a change of basis 
\begin{equation}
    c_{\tau} = \frac{1}{\sqrt{\beta}} \sum_\omega e^{-i\omega\tau} c_\omega,\quad\quad\quad\quad \bar{c}_\tau = \frac{1}{\sqrt{\beta}} \sum_\omega e^{i\omega\tau} \bar{c}_\omega.
\end{equation}
We can compute, if we write $S=S_1+S_2,$
\begin{align}
    S_1 &= -\frac{i}{\beta} \int_0^\beta \left(\sum_\omega  e^{i\omega\tau} \bar{c}_\omega\right)\left(\sum_\omega e^{-i\omega\tau}\omega c_\omega\right)  \dd{\tau} \\ 
    &=-i \left(\sum_\omega \omega \bar{c}_\omega c_\omega\right).
\end{align}
and 
\begin{align}
    S_2 &= \epsilon \sum_\omega \bar{c}_\omega c_{\omega},
\end{align}
so 
\begin{equation}
    S_0 =  \sum_\omega (\epsilon - i\omega) \bar{c}_\omega c_\omega
\end{equation}
Here, we used the fact that 
\begin{equation}
    \int_0^\beta e^{i(\omega-\omega')\tau}\dd{\tau} = \beta\delta_{\omega\omega'}.
\end{equation}
This is equivalent to diagonalizing a matrix (i.e. decoupling it) since we're now summing over one index. Using this, we can compute 
\begin{align}
    \langle \bar{c}_\omega \bar{c}_\omega\rangle &= \frac{\int \bar{c}_\omega c_\omega e^{-S_0} \mathcal{D}\bar{c}\mathcal{D}c}{\int e^{-S_0} \mathcal{D}\bar{c}\mathcal{D}c} \\
    &= \frac{\int \bar{c}_\omega c_\omega \prod_{\omega'} e^{(i\omega'-\epsilon)\bar{c}_{\omega'}c_{\omega'}}\mathcal{D}\bar{c}\mathcal{D}c}{\int \prod_{\omega'} e^{(i\omega'-\epsilon)\bar{c}_{\omega'}c_{\omega'}} \mathcal{D}\bar{c}\mathcal{D}c}.
\end{align}
But $\mathcal{D}c = \prod_{\omega}\dd{c_\omega}.$ One of the $\omega$ in the numerator will be the same as one of the $\omega'$ in the product, so this is equal to 
\begin{align}
    \langle \bar{c}_\omega \bar{c}_\omega\rangle &= \frac{\left(\bar{c}_\omega c_\omega e^{(i\omega-\epsilon)\bar{c}_\omega c_\omega} \dd{\bar{c}_\omega}\dd{c_\omega}\right)\left(\prod_{\omega'\neq \omega} \int e^{(i\omega'-\epsilon) \bar{c}_\omega' c_\omega'} \dd{\bar{c}_{\omega'}}\dd{{c}_{\omega'}}\right)}{\prod_{\omega'} \int e^{(i\omega'-\epsilon) \bar{c}_\omega' c_\omega'} \dd{\bar{c}_{\omega'}}\dd{{c}_{\omega'}}} \\ 
    &= \frac{\left(\bar{c}_\omega c_\omega e^{(i\omega-\epsilon)\bar{c}_\omega c_\omega} \dd{\bar{c}_\omega}\dd{c_\omega}\right)}{ \int e^{(i\omega-\epsilon) \bar{c}_\omega c_\omega} \dd{\bar{c}_{\omega}}\dd{{c}_{\omega}}} \\ 
    &= \frac{\int \bar{c} c (1+a\bar{c}c)}{a} \\ 
    &= \frac{1+0}{a} \\ 
    &= \frac{1}{i\omega-\epsilon},
\end{align}
where $a = i\omega - \epsilon,$ and we used Grassman number relationships. This is a \emf{Green function.} More generally, we have electrons that have a location and a spin. The location can be Fourier transformed into momentum:
\begin{equation}
    c_{i\sigma} = \frac{1}{\sqrt{N_s}} \sum_{\bm{k}} e^{i\bm{k}\cdot \bm{R}_i}c_{\bm{k}\sigma}.
\end{equation}
In general, the action is given by 
\begin{equation}
    S_0 = \sum_{\bm{k},\omega,\sigma}(-i\omega + \xi_{\bm{k}}) \bar{c}_{\bm{k}\omega\sigma}c_{\bm{k}\omega\sigma},
\end{equation}
where $\xi_{\bm{k}}$ is given by some other Fourier transform.
\begin{example}
    Consider applying Wick's Theorem to the following:
    \begin{align}
        \langle \bar{c}_{k-p,\uparrow} \bar{c}_{p,\downarrow}c_{k'-p',\downarrow}c_{p',\uparrow}\rangle &= \langle \bar{c}_{k-p, \uparrow} \bar{c}_{p,\downarrow}\rangle_0 \langle c_{k'-p',\downarrow}c_{p',\uparrow}\rangle_0 + \langle \bar{c}_{k-p, \uparrow} c_{k'-p',\downarrow}\rangle_0 \langle \bar{c}_{p,\downarrow}c_{p',\uparrow}\rangle_0 + \langle \bar{c}_{k-p, \uparrow} c_{p',\uparrow}\rangle_0 \langle \bar{c}_{p,\downarrow}c_{k'-p',\downarrow}\rangle_0 \\
        &= \langle \bar{c}_{k-p, \uparrow} c_{p',\uparrow}\rangle_0 \langle \bar{c}_{p,\downarrow}c_{k'-p',\downarrow}\rangle_0 \\ 
        &= \frac{1}{ip_0'-\xi_{p'}} \frac{1}{ip_0-\xi_p} \delta_{k-p,p'}\delta_{p,k'-p'}.
    \end{align}
\end{example}
Something like the above example will show up when we do the cumulant expansion, which we will now do. Recall that 
\begin{equation}
    S_{\text{eff}} = \int_0^\beta \sum_i \frac{|\Delta_{i\tau}|^2}{U} \dd{\tau} - \log\langle e^{-S'}\rangle_0,
\end{equation}
and we can perform the cumulant expansion on $\Delta S_{\text{eff}} = -\log \langle e^{-S'}\rangle_0.$ We can perform the Fourier transform 
\begin{align}
    \Delta_{i\tau} &= \frac{1}{\beta N_s} \sum_{k,\omega} e^{-i\tau \omega}e^{i\bm{k}\cdot \bm{R}_i} \Delta_{\bm{k},\omega} \\ 
    c_{i\tau\sigma} &= \frac{1}{\sqrt{N_s\beta}}\sum_{k,\omega}e^{-i\omega \tau}e^{i\bm{k}\cdot \bm{R}_i} c_{\bm{k}\omega\sigma}.
\end{align}
This gives us 
\begin{equation}
    S' = \frac{1}{\beta N_s}\sum_{k,p} \left(\Delta_k \bar{c}_{k-p,\uparrow}\bar{c}_{p,\downarrow} + \Delta^*_k c_{k-p,\downarrow}c_{p,\uparrow}\right),
\end{equation}
where things are 4-vectors whenever it makes sense. The cumulant expansion will then give us 
\begin{align}
    \Delta S_{\text{eff}} &= - \log \langle e^{-S'}\rangle_0 \\ 
    &=  -\sum_{n=1}^{\infty} \frac{(-1)^n}{n!}\langle (S')^n\rangle_{0,c} \\ 
    &= \langle S'\rangle_0 - \frac{1}{2}\langle (S')^2 \rangle_0 + \frac{1}{2}\langle S'\rangle_0^2.
\end{align}
We can compute,
\begin{align}
    \langle S'\rangle_0 = \frac{1}{\beta N_s}\sum_{k,p}\Delta_k \langle \bar{c}_{k-p,\uparrow} \bar{c}_{p,\downarrow}\rangle + \Delta^*_k \langle c_{k-p,\downarrow}c_{p,\uparrow}\rangle = 0 .
\end{align}
Therefore, we have 
\begin{equation}
    \Delta S_{\text{eff}} = - \frac{1}{2}\langle S'^2\rangle_0.
\end{equation}
We can compute,
\begin{align}
    (S')^2 &= \frac{1}{\beta^2N^2} \sum_{k,p,k',p'} \left(\sum_k \bar{c}_{k-p,\uparrow}\bar{c}_{p,\downarrow} + \Delta^*_k c_{k-p,\downarrow}c_{p,\uparrow}\right) \left(\sum_{k'} \bar{c}_{k'-p',\uparrow}\bar{c}_{p',\downarrow} + \Delta^*_k c_{k'-p',\downarrow}c_{p',\uparrow}\right) \\
    \langle S'^2\rangle_0 &= \frac{1}{\beta^2N^2}\sum_{k,k'} \langle \Delta_k \bar{c}_{k-p,\uparrow}\bar{c}_{p,\downarrow}\Delta^*_k c_{k'-p',\downarrow}c'_{p',\uparrow}\rangle_0 + \frac{1}{\beta^2N^2}\sum_{k,k'} \langle \Delta^*_k c_{k-p,\downarrow}c_{p,\uparrow}\Delta_{k'} \bar{c}_{k'-p',\uparrow}\bar{c}_{p',\downarrow}\rangle_0,
\end{align}
where note that the other two terms are zero. However, we have already figured out what these two terms are, using Wick's Theorem!
\newpage
d
\end{document}