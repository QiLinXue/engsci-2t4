\section{Differentiability}
\begin{itemize}
    \item We look at how we can define derivatives in multi-variable functions, specifically in $\mathbb{R}^n$.
    \begin{definition}
        $f:\mathbb{R}^n\rightarrow \mathbb{R}^m$ is differentiable at $a$ if and only if
        \begin{equation}
            f(a+h) = f(a) + Lh + o(h)
        \end{equation}
        where $o(h) = \{e:\mathbb{R}^n\rightarrow \mathbb{R}^m : e(0)=0, \lim_{h\to 0} \frac{e(h)}{|h|} = 0\}$ and $L$ is a linear transformation, which can be written as a matrix. We have abused some notation here.
    \end{definition}
    \item Some facts: 
    \begin{itemize}
        \item If $f$ is constant, $f(x)=c$ for all $x$, then $F$ is differentiable anywhere in its domain, and $f'=0.$
        \begin{proof}
            We want to write
            \begin{equation}
                f(a+h) = f(a) + Lh + o(h). 
            \end{equation}
            If we let $L=0$ and $o(h)=0$, then we are just left with $C=C.$
        \end{proof}
        \item If $F$ is linear, say $F=L:\mathbb{R}^n\rightarrow \mathbb{R}^m,$ then $F$ is differentiable and $Df(a)=f.$
        \begin{proof}
            Since $f$ is linear, we have 
            \begin{equation}
                f(a+h) = f(a) + f(h) + 0
            \end{equation}
            Therefore, $Df(a)=f.$
        \end{proof}
        ]item Consider $s:\mathbb{R}^2 \rightarrow \mathbb{R}$ such that $s(x,y)=x+y.$ We have that $s$ is differentiable and $s'=s.$
        
        Note that the common name for this function is $+$, therefore $+'=+=\begin{pmatrix}
            1 & 1
        \end{pmatrix}$
    \end{itemize}
    \item \textbf{Claim:} If $f:\mathbb{R}^n\rightarrow \mathbb{R}^m$ and $g:\mathbb{R}^n\rightarrow \mathbb{R}^m$ are differentiable at $a$, then so is $f+g$ and $(f+g)'=f'+g'.$
    \begin{proof}
        Since $f$ and $g$ are differentiable, 
        \begin{align}
            f(a+h)&=f(a)+f'(a)h+e_1(h) \\ 
            g(a+h)&=g(a)+g'(a)h+e_2(h)
        \end{align}
        Therefore, 
        \begin{align}
            (f+g)(a+h) &= f(a+h)+g(a+h) \\
            &= f(a)+g(h)+(f'(a)+g'(a))h + e_1(h) + e_2(h) \\ 
            &= (f+g)(h) + (f'(a)+g'(a))h + e_1(h)+e_2(h)
        \end{align}
        Since $o(h)$ is a vector space, we have $e_1(h)+e_2(h) \in o(h).$
    \end{proof}
    \begin{theorem}
        Suppose $F:\mathbb{R}^n\rightarrow \mathbb{R}^m$ is differentiable at $a$ and $g:\mathbb{R}^m\rightarrow \mathbb{R}^p$ is differentiable at $\bar{a}=f(a)$, then $(g\circ f):\mathbb{R}^n\rightarrow \mathbb{R}^p$ is differentiable at $a$ and 
        \begin{equation}
            D(g\circ f)(a) = (Dg)(f(a)) \cdot (Df)(a).
        \end{equation}
        Alternatively, 
        \begin{equation}
            (g\circ f)'(a) = g'(f(a)) \cdot f'(a)
        \end{equation}
        where $\cdot$ denotes matrix multiplication. Note that we could restrict $f$ to some open $A \ni a$ and $g$ to some open $B \supset f(A).$
    \end{theorem}
    \begin{proof}
        We know that $f(a+h)=f(a)+f'(a)h+e_1(h)$ and $g(\bar{a}+\bar{h})=g(\bar{a})+g'(\bar{a})\bar{h} + e_2(\bar{h}).$ Now, 
        \begin{align}
            (g\circ f)(a+h) &= g(f(a+h)) \\ 
            &= g(\underbrace{f(a)}_{\bar{a}}+\underbrace{f'(a)h+e_1(h)}_{\bar{h}}) \\ 
            &=g(\bar{a}) + g'(\bar{a})\bar{h} + e_2(\bar{h})
        \end{align}
        so the derivative is 
        \begin{equation}
            g'(f(a))f'(a)h.
        \end{equation}
        It is easy to check that $g'(\bar{a})e_1(h)+e_2(f'(a)h+e_1(h)) \in o(h).$ We can do this with a few lemmas.
    \end{proof}
    \begin{lemma}
        If $A$ is a matrix and $e\in o(h)$ then $Ae \in o(h).$ Note that the two $o(h)$ are different since they may live in different dimensions.        
    \end{lemma}
    \begin{lemma}
        If for small $h,$ there exists some $C$ such that $|\lambda(h)| < C|h|$ where $\lambda(h) = Lh + e(h).$ Then $e\circ \lambda$ is $o(h)$.
    \end{lemma}
    Showing these two lemmas are true will allow us to finish the proof.
    \begin{proof}
        Note that the first lemma is true since we can find constant $C_1$ such that $|Ah| \le C_1|h|.$ This is true for all $h$ and this is proven in assignment 1.
        \vspace{2mm}

        First, we show that $(e\circ \lambda)(0) = 0$ and it remains to show that 
        \begin{equation}
            \lim_{h\to 0} \frac{e(\lambda(h))}{|h|} = 0,
        \end{equation}
        which is equivalent to saying that $\forall \epsilon > 0, \exists \delta > 0$ s.t. $|h| <\delta \implies |e(\lambda(h))| \le \epsilon |h|.$ To do this, suppose $|\lambda(h)| \le C|h|$ on $B_{\delta_2}(0)$ and $|e(y)| \le \frac{\epsilon}{C}|y|$ on $B_{\delta_1}(0).$ Set $\delta=\min\left\{\frac{\delta_1}{C},\delta_2\right\}$ and get for $|h| < \delta$, 
        \begin{equation}
            e(\lambda(h)) \le \frac{e}{c}|\lambda(h)| \le c|h|
        \end{equation}
    \end{proof}
    \begin{example}
        Suppose we wish to compute the differential of $x/y$ using the chain rule. Let $g(x,y) = \frac{x}{y}$ where $g:\mathbb{R}^2_{y\neq 0} \rightarrow \mathbb{R}$. We can then write:
        \begin{equation}
            \frac{x}{y} = e^{\ln x - \ln y}
        \end{equation}
        so this is the mapping 
        \begin{equation}
            \mathbb{R}^2_{x,y\neq 0} \xrightarrow[f_1]{\begin{pmatrix}
                x \\ y
            \end{pmatrix}\mapsto \begin{pmatrix}
                \ln x \\ \ln y
            \end{pmatrix}} \mathbb{R}^2_{\xi,\eta} \xrightarrow[f_2]{\begin{pmatrix}
                \xi \\ \eta
            \end{pmatrix}\mapsto \begin{pmatrix}
                \xi - \gamma
            \end{pmatrix}}\mathbb{R}_{\gamma} \xrightarrow[f_3]{\gamma \mapsto e^\gamma}\mathbb{R}.
        \end{equation}
        We can then write $g = f_3 \circ (f_2 \circ f_1).$ Therefore: 
        \begin{equation}
            g' = f_3'(f_2(f_1(x,y))) \cdot f_2'(f_1(x,y)) \cdot f_1'(x,y)
        \end{equation}
        We can start computing the derivatives. We have
        \begin{equation}
            f_3' = \begin{pmatrix}
                e^\gamma
            \end{pmatrix} = \begin{pmatrix}
                e^{\ln x-\ln y}
            \end{pmatrix} = \begin{pmatrix}
                x/y
            \end{pmatrix}
        \end{equation}
        and 
        \begin{equation}
            f_2' = (-)' = (-) = \begin{pmatrix}
                1 & -1
            \end{pmatrix}
        \end{equation}
        and using the claim from directly after,
        \begin{equation}
            f_1' = \begin{pmatrix}
                \left[f_{11}(x,y)=\ln x\right]' \\ 
                \left[f_{12}(x,y)=\ln y\right]'
            \end{pmatrix} = \begin{pmatrix}
                1/x & 0 \\ 
                0 & 1/y
            \end{pmatrix}
        \end{equation}
        Note that $f_{11}$ is itself a composition: 
        \begin{equation}
            f_{11}: \mathbb{R}_{x,y}^2 \xrightarrow[\pi_1]{\begin{pmatrix}
                x \\ y
            \end{pmatrix}\mapsto \begin{pmatrix}
                x
            \end{pmatrix}} \mathbb{R}_x \xrightarrow[\ln]{x \mapsto \ln x} \mathbb{R}.
        \end{equation}
        Therefore: 
        \begin{equation}
            f_11' = \pi_1'(\ln x) \cdot \ln'(x) = \pi_1 \cdot \frac{1}{x} = \begin{pmatrix}
                \frac{1}{x} & 0.
            \end{pmatrix}
        \end{equation}
        Putting it altogether,
        \begin{equation}
            g' = \begin{pmatrix}
                x/y
            \end{pmatrix}\begin{pmatrix}
                1 & -1
            \end{pmatrix}\begin{pmatrix}
                1/x & 0 \\ 
                0 & 1/y
            \end{pmatrix} = \begin{pmatrix}
                x/y
            \end{pmatrix}\begin{pmatrix}
                1/x - 1/y
            \end{pmatrix} = \begin{pmatrix}
                \frac{1}{y} - \frac{x}{y^2}
            \end{pmatrix}
        \end{equation}
    \end{example}
    \item \textbf{Claim 1:} Suppose $F=\begin{pmatrix}
        F_1 \\ \vdots \\\ F_m
    \end{pmatrix}$ so each $F_i:\mathbb{R}^n \rightarrow \mathbb{R}$ and $a = \begin{pmatrix}
        a_1 \\ \vdots \\ a_n
    \end{pmatrix}.$ $F$ is differentiable if and only if each of $F_i$ is differentiable, and then
    \begin{equation}
        F'(a) = \begin{pmatrix}
            F_1'(a) \\ 
            \vdots \\ 
            F_m'(a)
        \end{pmatrix}
    \end{equation} 
    \begin{proof}
        $\impliedby:$ Look at: 
        \begin{align}
            f(a+h)-f(a) &= \begin{pmatrix}
                f_1(a+h)-f_1(a) \\ \vdots \\ f_m(a+h) - f_m(a)
            \end{pmatrix} \\ 
            &= \begin{pmatrix}
                f_1'(a)\cdot h + e_1(h) \\ 
                \vdots \\ 
                f_m'(a) \cdot h + e_m(h)
            \end{pmatrix} \\ 
            &= \begin{pmatrix}
                f_1'(a) \\ \vdots \\ f_m'(a)
            \end{pmatrix}\cdot h + \begin{pmatrix}
                e_1(h) \\ \vdots \\ e_m(h)
            \end{pmatrix}
        \end{align}
        All that remains to show is that a column vector consisting of tiny functions is itself a tiny function. Finally, the other implication is easy.
    \end{proof}
    \item Some exercises: 
    \begin{itemize}
        \item Compute $P(x,y) = x\cdot y$ using chain rule and directly.
        \item Show that 
        \begin{equation}
            \left(\frac{f(t)}{g(t)}\right)' = \frac{f'g-g'f}{g^2}
        \end{equation}
    \end{itemize}
    \begin{definition}
        Suppose $f:\mathbb{R}^n_{x_i}\rightarrow \mathbb{R}.$ Define $D_if = \partial_i f = \partial_{x_i} = \frac{\partial f}{\partial x_i} = f_i = f_{x_i}.$
    \end{definition}
    \begin{theorem}
        $f:\mathbb{R}^n\rightarrow \mathbb{R}$, $a\in \mathbb{R}^n$, $(D_if)(x)$ exists and are continuous near $a$, implies that $f$ is differentiable at $a$ and 
        \begin{equation}
            f'(a) = (\partial_1 f(a), \dots, \partial_n f(a))
        \end{equation}
    \end{theorem}
    \begin{proof}
        Set $b_1=(a_1+h_1,\dots,a_i+h_i,\dots,a_n)$ so $b_0=a,b_n=a+h$. Then 
        \begin{align}
            e(h) &= f(a+h)-f(a) - \sum (\partial_i f)(a) \cdot h_i \\ 
            &= f(b_n)-f(b_0)-\sum (\partial_i f)(a) \cdot h_i \\ 
            &= \sum f(b_i)-f(b_{i-1})-\sum (\partial_i f)(a) \cdot h_i
        \end{align}
        which by the mean value theorem, there exists $c_i$ in between $b_{i-1}$ and $b_{i}$ such that 
        \begin{equation}
            c_i = (a_1+h_1, a_{i-1}+h_{hi-1},\gamma_i, a_{i+1},a_n)
        \end{equation}
        where $\gamma_i \in (a_i, a_i + h_i)$. Then:
        \begin{equation}
            \sum (D_i F)(c_i)h_i - (D_if)(a)h_i = \sum_{i=1}^n ((D_if)(c_i)-(D_if)(a))h_i = e(h)
        \end{equation} 
        We want to now compute 
        \begin{align}
            \lim_{h\to 0} \frac{|e(h)|}{|h|} &= \lim_{h\to 0} \frac{\left|\sum (D_i F)(c_i)h_i - (D_if)(a)h_i = \sum_{i=1}^n ((D_if)(c_i)-(D_if)(a))h_i\right|}{|h|} 
        \end{align}
        and we need to show that 
        \begin{equation}
            0 = \lim_{h\to 0} \sum |D_if(c_i)-(D_if)(a)| \cdot \frac{|h_i|}{h}
        \end{equation}
        As $h\to 0$, $c_i\to a$. By continuity of $\partial_i f$, we know $D_if(c_i)-D_i(f_a)\to 0$, so the above limit is zero.
    \end{proof}
    \item Aside: Let $R\subset \mathbb{R}^n$ be a rectangle, $f:R \to \mathbb{R}^m$ and is differentiable, assume there is some $M$ such that for all $i,j$, and $x\in \Int R$. $|D_i F_j(x)| \le M$. Then $\forall x,y,\in R$,
    \begin{equation}
        |f(x)-f(y)| \le nmM|x-y|
    \end{equation}
    \subsection{Second Derivative}
    \item These notations are equivalent:
    \begin{equation}
        D_j(D_if)=D_{i,j}f = \partial_{i,j}f = \frac{\partial^2}{\partial x_i \partial x_j}f
    \end{equation}
    \begin{theorem}
        If $D_{i,j}f$ and $D_{j,i}f$ exist and are continuous near $a$, then they are equal.
    \end{theorem}
    \item We can generalize this theorem to higher dimensions as well.
    \begin{definition}
        If $A \subset \mathbb{R}^n$, then $C^\infty (A) = \{F: A \rightarrow \mathbb{R}^m:\text{ all partial derivatives of all orders exist}\}$
    \end{definition}
    \subsection{Inverse Function Theorem}
    \begin{theorem}
        Given $a\in A \subset \mathbb{R}^n$
    \end{theorem}
    \item Oftentimes we use the phrase: ``statement`` is true '''``near'' a point $a$. What this means is that there exists $x_1,x_2 \in B_r(a)$ such that ``statement'' is true.
    \item An \textbf{All Scale Fidelity} of $c$ means that $\exists r > 0$ such that 
    \begin{equation}
        |(f(x_1)-f(x_2))-(x_1-x_2)| \le \frac{1}{c} |x_1-x_2|
    \end{equation}
    Alternatively, we can let $\alpha = (x_1-x_2)$ and $\beta=f(x_1)-f(x_2)$, so the all scale fidelity becomes 
    \begin{equation}
        |\alpha-\beta| \le \frac{1}{c} |\alpha| \le \frac{1}{c}|\beta| + |alpha-\beta|
    \end{equation}
    which gives
    \begin{equation}
        |\alpha-\beta| \le \frac{1}{c-1}|\beta|
    \end{equation}
    \subsection{Implicit Function Theorem}

\end{itemize}