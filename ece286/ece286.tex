\documentclass{article}
\usepackage{qilin}
\tikzstyle{process} = [rectangle, rounded corners, minimum width=1.5cm, minimum height=0.5cm,align=center, draw=black, fill=gray!30, auto]
\title{ECE286: Stats}
\author{QiLin Xue}
\date{Winter 2022}
\usepackage{mathrsfs}
\usetikzlibrary{arrows}
\usepackage{stmaryrd}
\usepackage{accents}
\newcommand{\ubar}[1]{\underaccent{\bar}{#1}}
\usepackage{pgfplots}
\numberwithin{equation}{section}

\begin{document}

\maketitle
\tableofcontents
\newpage
\section{Cumulative Distribution Functions}
Given some PDF $f(x)$, the \emf{cumulative distribution function} (CDF) is defined as:  
\begin{equation}
    CDF(X) = \int_{-\infty}^x f(t) \dd{t}
\end{equation}
where $CFD(\infty)=1.$ Recall that given some function $g(x)$, we have 
\begin{equation}
    \int_A^C g(x) \dd{x} = \int_A^B g(x) \dd{x} + \int_B^C g(x) \dd{x}.
\end{equation}
Therefore, if we have some random value $x$ which has a PDF $f(x)$. Then:
\begin{equation}
    P(A\le X \le B) = F(B) - F(A).
\end{equation}
Let us now determine the \emf{normal CDF.} Recall that
\begin{equation}
    n(x,\mu,\sigma) = \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}}. % autocompletes
\end{equation}
And therefore the CDF is given by
\begin{equation}
    \Phi(x, \mu, \sigma) = \int_{-\infty}^x n(t;\mu,\sigma) \dd{t},
\end{equation}
so $P(A\le X\le B, \mu, \sigma) = \Phi(B, \mu, \sigma) - \Phi(a, \mu, \sigma).$ Note that $\Phi$ cannot be written in terms of elementary functions. In practice, we will use tables to evaluate this. However, we don't want tables for every $\mu$ and $\sigma$, so we will parametrize it by a single variable and relate it to the \emf{CDF for a standard normal RV},
\begin{equation}
    \Phi(x) = \int_{-\infty}^t n(t;0,1)\dd{t}.
\end{equation}
Suppose $X$ has PDF $n(x; \mu, \sigma).$ Let $z = \frac{x-\mu}{\sigma}.$ Consider:
\begin{align}
    P(X \le x) &= \int_{-\infty}^x n(t; 0, 1) \dd{t} \\ 
    &= \int_{-\infty}^x \frac{1}{\sqrt{2\pi}\sigma} e^{\frac{-(t-\mu)^2}{2\sigma^2}}\dd{t} \\ 
    &= \int_{-\infty}^{(x-\mu)/\sigma} \frac{1}{\sqrt{2\pi}\sigma} e^{-s^2/2} \dd{t} \\ 
    &= \int_{-\infty}^{(x-\mu)/\sigma} n(s; 0, 1) \dd{s} \\ 
    &= P\left(\frac{x-\mu}{\sigma}\right) \\
    &= P\left(Z \le \frac{x-\mu}{\sigma}\right).
\end{align}
\textit{Note:} In MATLAB, the code for $\Phi$ is \verb#normcdf#.
\section{Binomial PMF}
Recall that when we have $n$ coin flips, $p$ is the probability of heads, and $X$ is the number of heats. Recall that the \emf{probability mass function} is
\begin{equation}
    b(x; n, p) = \binom{n}{x} p^x (1-p)^{n-x}.
\end{equation}
The mean is $np$ and the variance is $np(1-p)$. Then:
\begin{equation}
    z = \frac{x-np}{\sqrt{np(1-p)}}.
\end{equation}
A preview of the \emf{central limit theorem} is that in the limiting case of $n\rightarrow\infty$, the distribution of $X$ is the normal distribution.
\section{Gamma Function}
The \emf{gamma function} is defined as:
\begin{equation}
    \Gamma(z) = \int_0^\infty x^{\alpha-1}e^{-x}\dd{x}
\end{equation}
for $\alpha > 0$. IT has the following properties:
\begin{itemize}
    \item $\Gamma\left(\frac{1}{2}\right) = \sqrt{\pi}.$
    \item $\Gamma(n) = (n-1)!$ where $n\in \mathbb{N}.$
\end{itemize}
The \emf{gamma distribution} is
\begin{equation}
    f(x; \alpha, \beta) = \begin{cases}
        \frac{1}{\beta^\alpha \Gamma(\alpha)}x^{\alpha-1}e^{-x/\beta} & x > 0 \\ 
        0& \text{otherwise}
    \end{cases}
\end{equation}
The mean is $\mu = \alpha\beta$ and the variance is $\sigma = \alpha\beta^2.$

The gamma distribution is special since many other distributions can be written as it. For example, the \emf{chi-squared} distribution $\chi^2$ is
\begin{equation}
    f(x;v) = f_\text{gamma}(x;v/2,2) = \begin{cases}
        \frac{1}{2^{v/2}\Gamma(v/2)}x^{v/2-1}e^{-x/2} & x > 0 \\
        0 & \text{otherwise}
    \end{cases}.
\end{equation}
The \emf{exponential distribution is}
\begin{equation}
    f(x; \beta) = f_\text{gamma}(x;1,\beta)\begin{cases}
        \frac{1}{\beta}e^{-x/\beta} & x > 0 \\
        0 & \text{otherwise}
    \end{cases}.
\end{equation}
Let us relate this to the poisson PMF,
\begin{equation}
    p(x;\lambda) = \frac{e^{-\lambda}\lambda^x}{x!}.
\end{equation}
We can interpret this by setting $\lambda = rt$, where $r$ is the rate of arrivals and $t$ is the length of the interval. Therefore, the probability of no arrivals in an interval of length $t$ is:
\begin{equation}
    p(0;rt) = e^{-rt}.
\end{equation}
Let $Y$ be a RV for the time to first arrival. Then:
\begin{equation}
    P(Y>t) = e^{-rt}.
\end{equation}
Therefore,
\begin{equation}
    P(Y \le t) = 1 - e^{-rt} = F(t),
\end{equation}
so the PDF of $Y$ is
\begin{equation}
    f(t) = \frac{d}{dt}F(t) = re^{-rt}.
\end{equation}
\section{Memoryless Property}
Suppose $X$ is exponential with $\beta>0$. Then:
\begin{equation}
    P(X>s+t | X>s) = \frac{P(X > s + t \cap X >s)}{P(X > s)} = \frac{P(X > s+t)}{P(x>s)}.
\end{equation}
This is equal to 
\begin{align}
    &= \frac{\int_{s+t}^{\infty} \frac{1}{\beta}e^{-x/\beta} \dd{x}}{\int_s^\infty \frac{1}{\beta} e^{-x/\beta} \dd{x}} \\ 
    &= \frac{e^{-(s+t)/\beta}}{e^{-s/\beta}} \\ 
    &= e^{-t/\beta} \\ 
    &= P(X>1).
\end{align}
\end{document}
